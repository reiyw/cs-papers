title,authors,arxiv_url,abstract
A Bilingual Generative Transformer for Semantic Sentence Embedding,"['John Wieting', 'Graham Neubig', 'Taylor Berg-Kirkpatrick']",http://arxiv.org/abs/1911.03895v1,"Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks -- contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity."
A Centering Approach for Discourse Structure-aware Coherence Modeling,"['Sungho Jeon', 'Michael Strube']",,
A Computational Approach to Understanding Empathy Expressed in Text-Based Mental Health Support,"['Ashish Sharma', 'Adam Miner', 'David Atkins', 'Tim Althoff']",http://arxiv.org/abs/2009.08441v1,"Empathy is critical to successful mental health support. Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts. Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial. In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms. We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations. We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales). We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions. Experiments demonstrate that our approach can effectively identify empathic conversations. We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback."
A Dataset for Tracking Entities in Open Domain Procedural Text,"['Niket Tandon', 'Keisuke Sakaguchi', 'Bhavana Dalvi', 'Dheeraj Rajagopal', 'Peter Clark', 'Michal Guerquin', 'Kyle Richardson', 'Eduard Hovy']",,
A Diagnostic Study of Explainability Techniques for Text Classification,"['Pepa Atanasova', 'Jakob Grue Simonsen', 'Christina Lioma', 'Isabelle Augenstein']",http://arxiv.org/abs/2009.13295v1,"Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques."
A Joint Multiple Criteria Model in Transfer Learning for Cross-domain Chinese Word Segmentation,"['Kaiyu Huang', 'Degen Huang', 'Zhuang Liu', 'Fengran Mo']",,
A Knowledge-Aware Sequence-to-Tree Network for Math Word Problem Solving,"['Qinzhuo Wu', 'Qi Zhang', 'Jinlan Fu', 'Xuanjing Huang']",,
A Knowledge-driven Generative Model for Multi-implication Chinese Medical Procedure Entity Normalization,"['Jinghui Yan', 'Yining Wang', 'Lu Xiang', 'Yu Zhou', 'Chengqing Zong']",,
A Massive Collection of Cross-Lingual Web-Document Pairs,"['Ahmed El-Kishky', 'Vishrav Chaudhary', 'Francisco Guzm√°n', 'Philipp Koehn']",http://arxiv.org/abs/1911.06154v1,"Cross-lingual document alignment aims to identify pairs of documents in two distinct languages that are of comparable content or translations of each other. Small-scale efforts have been made to collect aligned document level data on a limited set of language-pairs such as English-German or on limited comparable collections such as Wikipedia. In this paper, we mine twelve snapshots of the Common Crawl corpus and identify web document pairs that are translations of each other. We release a new web dataset consisting of 54 million URL pairs from Common Crawl covering documents in 92 languages paired with English. We evaluate the quality of the dataset by measuring the quality of machine translations from models that have been trained on mined parallel sentence pairs from this aligned corpora and introduce a simple yet effective baseline for identifying these aligned documents. The objective of this dataset and paper is to foster new research in cross-lingual NLP across a variety of low, mid, and high-resource languages."
A Matter of Framing: The Impact of Linguistic Formalism on Probing Results,"['Ilia Kuznetsov', 'Iryna Gurevych']",http://arxiv.org/abs/2004.14999v1,"Deep pre-trained contextualized encoders like BERT (Delvin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings."
A Method for Building a Commonsense Inference Dataset based on Basic Events,"['Kazumasa Omura', 'Daisuke Kawahara', 'Sadao Kurohashi']",,
A Multi-Task Incremental Learning Framework with Category Name Embedding for Aspect-Category Sentiment Analysis,"['Zehui Dai', 'Cheng Peng', 'Huajie Chen', 'Yadong Ding']",http://arxiv.org/abs/2010.02784v1,"(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines."
A Predicate-Function-Argument Annotation of Natural Language for Open-Domain Information Expression,"['Mingming Sun', 'Wenyue Hua', 'Zoe Liu', 'Xin Wang', 'kangjie zheng', 'Ping Li']",,
A Probabilistic End-To-End Task-Oriented Dialog Model with Latent Belief States towards Semi-Supervised Learning,"['Yichi Zhang', 'Zhijian Ou', 'Min Hu', 'Junlan Feng']",http://arxiv.org/abs/2009.08115v2,"Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems. However, training belief trackers often requires expensive turn-level annotations of every user utterance. In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning. We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs. Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework. Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES. In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales. In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines. Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ."
A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?,"['Hongyu Lin', 'Yaojie Lu', 'Jialong Tang', 'Xianpei Han', 'Le Sun', 'Zhicheng Wei', 'Nicholas Jing Yuan']",http://arxiv.org/abs/2004.12126v1,"Fine-tuning pretrained model has achieved promising performance on standard NER benchmarks. Generally, these benchmarks are blessed with strong name regularity, high mention coverage and sufficient context diversity. Unfortunately, when scaling NER to open situations, these advantages may no longer exist, and therefore raise the critical question of whether pretrained supervised models can still work well when facing these issues. As there is no currently available dataset to investigate this problem, this paper proposes to conduct randomization test on standard benchmarks. Specifically, we erase name regularity, mention coverage and context diversity respectively from the benchmarks, in order to explore their impact on the generalization ability of models. Moreover, we also construct a new open NER dataset that focuses on entity types with weak name regularity such as book, song, and movie. From both randomization test and empirical experiments, we draw the conclusions that 1) name regularity is vital for generalization to unseen mentions; 2) high mention coverage may undermine the model generalization ability and 3) context patterns may not require enormous data to capture when using pretrained supervised models."
A Spectral Method for Unsupervised Multi-Document Summarization,"['Kexiang Wang', 'Baobao Chang', 'Zhifang Sui']",,
A State-independent and Time-evolving Network with Applications to Early Rumor Detection,"['Rui Xia', 'Kaizhou Xuan', 'Jianfei Yu']",,
A Supervised Word Alignment Method based on Cross-Language Span Prediction using Multilingual BERT,"['Masaaki Nagata', 'Katsuki Chousa', 'Masaaki Nishino']",http://arxiv.org/abs/2004.14516v1,"We present a novel supervised word alignment method based on cross-language span prediction. We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence. As this is equivalent to a SQuAD v2.0 style question answering task, we then solve this problem by using multilingual BERT, which is fine-tuned on a manually created gold word alignment data. We greatly improved the word alignment accuracy by adding the context of the token to the question. In the experiments using five word alignment datasets among Chinese, Japanese, German, Romanian, French, and English, we show that the proposed method significantly outperformed previous supervised and unsupervised word alignment methods without using any bitexts for pretraining. For example, we achieved an F1 score of 86.7 for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised methods."
A Synset Relation-enhanced Framework with a Try-again Mechanism for Word Sense Disambiguation,"['Ming Wang', 'Yinglin Wang']",,
A Time-Aware Transformer based model for Suicide Ideation Detection on Social Media,"['Ramit Sawhney', 'Harshit Joshi', 'Saumya Gandhi', 'Rajiv Ratn Shah']",,
A Visually-grounded First-person Dialogue Dataset with Verbal and Non-verbal Responses,"['Hisashi Kamezawa', 'Noriki Nishida', 'Nobuyuki Shimizu', 'Takashi Miyazaki', 'Hideki Nakayama']",,
Accurate Word Alignment Induction from Neural Machine Translation,"['Yun Chen', 'Yang Liu', 'Guanhua Chen', 'Xin Jiang', 'Qun Liu']",http://arxiv.org/abs/2004.14837v1,"Despite its original goal to jointly learn to align and translate, prior researches suggest that the state-of-the-art neural machine translation model Transformer captures poor word alignment through its attention mechanism. In this paper, we show that attention weights do capture accurate word alignment, which could only be revealed if we choose the correct decoding step and layer to induce word alignment. We propose to induce alignment with the to-be-aligned target token as the decoder input and present two simple but effective interpretation methods for word alignment induction, either through the attention weights or the leave-one-out measures. In contrast to previous studies, we find that attention weights capture better word alignment than the leave-one-out measures under our setting. Using the proposed method with attention weights, we greatly improve over fast-align on word alignment induction. Finally, we present a multi-task learning framework to train the Transformer model and show that by incorporating GIZA++ alignments into our multi-task training, we can induce significantly better alignments than GIZA++."
Acrostic Poem Generation,"['Rajat Agarwal', 'Katharina Kann']",http://arxiv.org/abs/2010.02239v1,"We propose a new task in the area of computational creativity: acrostic poem generation in English. Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase. We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem's semantics should also relate to it, and 3) the poem should conform to a rhyming scheme. We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model. Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems. Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints. Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance."
Active Learning for BERT: An Empirical Study,"['Liat Ein-Dor', 'Alon Halfon', 'Ariel Gera', 'Eyal Shnarch', 'Lena Dankin', 'Leshem Choshen', 'Marina Danilevsky', 'Ranit Aharonov', 'Yoav Katz', 'Noam Slonim']",,
Adaptive Attentional Network for Few-Shot Knowledge Graph Completion,"['Jiawei Sheng', 'Shu Guo', 'Zhenyu Chen', 'Juwei Yue', 'Lihong Wang', 'Tingwen Liu', 'Hongbo Xu']",,
Adversarial Attack and Defense of Structured Prediction Models,"['Wenjuan Han', 'Liwen Zhang', 'Yong Jiang', 'Kewei Tu']",http://arxiv.org/abs/2010.01610v1,"Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years. However, most of the existing approaches focus on classification problems. In this paper, we investigate attacks and defenses for structured prediction tasks in NLP. Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input. To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task. Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate. We evaluate the proposed framework in dependency parsing and part-of-speech tagging. Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training."
Adversarial Self-Supervised Data Free Distillation for Text Classification,"['Xinyin Ma', 'Yongliang Shen', 'Gongfan Fang', 'Chen Chen', 'Chenghao Jia', 'Weiming Lu']",,
Adversarial Semantic Collisions,"['Congzheng Song', 'Alexander Rush', 'Vitaly Shmatikov']",,
Affective Event Classification with Discourse-enhanced Self-training,"['Yuan Zhuang', 'Tianyu Jiang', 'Ellen Riloff']",,
ALICE: Active Learning with Contrastive Natural Language Explanations,"['Weixin Liang', 'James Zou', 'Zhou Yu']",http://arxiv.org/abs/2009.10259v1,"Training a supervised neural network classifier typically requires many annotated training samples. Collecting and annotating a large number of data points are costly and sometimes even infeasible. Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides several bits of information. We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning. ALICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts. Then it extracts knowledge from these explanations using a semantic parser. Finally, it incorporates the extracted knowledge through dynamically changing the learning model's structure. We applied ALICE in two visual recognition tasks, bird species classification and social relationship classification. We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data. We found that adding 1 explanation leads to similar performance gain as adding 13-30 labeled training data points."
Alignment-free Cross-lingual Semantic Role Labeling,"['Rui Cai', 'Mirella Lapata']",,
Amalgamating Knowledge from Two Teachers for Task-oriented Dialogue System with Adversarial Training,"['Wanwei He', 'Min Yang', 'Rui Yan', 'Chengming Li', 'Ying Shen', 'Ruifeng Xu']",,
AmbigQA: Answering Ambiguous Open-domain Questions,"['Sewon Min', 'Julian Michael', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",http://arxiv.org/abs/2004.10645v2,"Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa."
An Analysis of Natural Language Inference Benchmarks through the Lens of Negation,"['Md Mosharaf Hossain', 'Venelin Kovatchev', 'Pranoy Dutta', 'Tiffany Kao', 'Elizabeth Wei', 'Eduardo Blanco']",,
An Effective Data Augmentation Method for Low-resource Tagging Tasks,"['BOSHENG DING', 'Linlin Liu', 'Lidong Bing', 'Canasai Kruengkrai', 'Thien Hai Nguyen', 'Shafiq Joty', 'Luo Si', 'Chunyan Miao']",,
An Embedding Model for Estimating Legislative Preferences from the Frequency and Sentiment of Tweets,"['Gregory Spell', 'Brian Guay', 'Sunshine Hillygus', 'Lawrence Carin']",,
An Empirical Investigation of Contextualized Number Prediction,"['Taylor Berg-Kirkpatrick', 'Daniel Spokoyny']",,
An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training,"['Kristjan Arumae', 'Qing Sun', 'Parminder Bhatia']",http://arxiv.org/abs/2010.00784v1,"Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33% drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods."
An Empirical Study of Generation Order for Machine Translation,"['William Chan', 'Mitchell Stern', 'Jamie Kiros', 'Jakob Uszkoreit']",http://arxiv.org/abs/1910.13437v1,"In this work, we present an empirical study of generation order for machine translation. Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies. We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders. Curiously, we find that for the WMT'14 English $\to$ German translation task, order does not have a substantial impact on output quality, with unintuitive orderings such as alphabetical and shortest-first matching the performance of a standard Transformer. This demonstrates that traditional left-to-right generation is not strictly necessary to achieve high performance. On the other hand, results on the WMT'18 English $\to$ Chinese task tend to vary more widely, suggesting that translation for less well-aligned language pairs may be more sensitive to generation order."
An Empirical Study of Hyperbole,"['Li Kong', 'Chuanyi Li', 'Jidong Ge', 'Bin Luo', 'Vincent Ng']",,
An Empirical Study on Large-Scale Multi-Label Text Classification including Few and Zero-Shot Labels,"['Ilias Chalkidis', 'Manos Fergadiotis', 'Sotiris Kotitsas', 'Prodromos Malakasiotis', 'Nikolaos Aletras', 'Ion Androutsopoulos']",http://arxiv.org/abs/2010.01653v1,"Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges. First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of LMTC datasets. Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity. Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization. Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks. Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains. We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs. Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWANs. Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce."
An Exploration of Arbitrary-Order Sequence Labeling via Energy-Based Inference Networks,"['Lifu Tu', 'Tianyu Liu', 'Kevin Gimpel']",http://arxiv.org/abs/2010.02789v1,"Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation. Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic. In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence. We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks. We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models. We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers. We also find high-order energies to help in noisy data conditions."
An Imitation Game for Learning Semantic Parsers from User Interaction,"['Ziyu Yao', 'Yiqi Tang', 'Wen-tau Yih', 'Huan Sun', 'Yu Su']",http://arxiv.org/abs/2005.00689v2,"Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstration when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstration, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and re-trains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at https://github.com/sunlab-osu/MISP."
An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,"['Bhargavi Paranjape', 'Mandar Joshi', 'John Thickstun', 'Hannaneh Hajishirzi', 'Luke Zettlemoyer']",http://arxiv.org/abs/2005.00652v2,"Decisions of complex language understanding models can be rationalized by limiting their inputs to a relevant subsequence of the original text. A rationale should be as concise as possible without significantly degrading task performance, but this balance can be difficult to achieve in practice. In this paper, we show that it is possible to better manage this trade-off by optimizing a bound on the Information Bottleneck (IB) objective. Our fully unsupervised approach jointly learns an explainer that predicts sparse binary masks over sentences, and an end-task predictor that considers only the extracted rationale. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on ERASER benchmark tasks demonstrate significant gains over norm-minimization techniques for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples) closes the gap with a model that uses the full input."
An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing,"['Martin Schmitt', 'Sahand Sharifzadeh', 'Volker Tresp', 'Hinrich Sch√ºtze']",http://arxiv.org/abs/1904.09447v3,"Knowledge graph (KG) schemas can vary greatly from one domain to another. Therefore supervised approaches to graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data, while adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and (2) is easy to adapt to new KG schemas. To this end, we present the first approach to fully unsupervised text generation from KGs and KG generation from text. Inspired by recent work on unsupervised machine translation, we serialize a KG as a sequence of facts and frame both tasks as sequence translation. By means of a shared sequence encoder and decoder, our model learns to map both graphs and texts into a joint semantic space and thus generalizes over different surface representations with the same meaning. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text$\leftrightarrow$graph tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives."
An Unsupervised Sentence Embedding Method by Mutual Information Maximization,"['Yan Zhang', 'Ruidan He', 'ZUOZHU LIU', 'Kwan Hui Lim', 'Lidong Bing']",http://arxiv.org/abs/2009.12061v1,"BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming. Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed. However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce. In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner. Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus. Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks. It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks."
Analogous Process Structure Induction for Sub-event Sequence Prediction,"['Hongming Zhang', 'Muhao Chen', 'Haoyu Wang', 'Yangqiu Song', 'Dan Roth']",,
Analyzing Individual Neurons in Pre-trained Language Models,"['Nadir Durrani', 'Hassan Sajjad', 'Fahim Dalvi', 'Yonatan Belinkov']",http://arxiv.org/abs/2010.02695v1,"While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled."
Analyzing Redundancy in Pretrained Transformer Models,"['Fahim Dalvi', 'Hassan Sajjad', 'Nadir Durrani', 'Yonatan Belinkov']",http://arxiv.org/abs/2004.04010v2,"Transformer-based deep NLP models are trained using hundreds of millions of parameters, limiting their applicability in computationally constrained environments. In this paper, we study the cause of these limitations by defining a notion of Redundancy, which we categorize into two classes: General Redundancy and Task-specific Redundancy. We dissect two popular pretrained models, BERT and XLNet, studying how much redundancy they exhibit at a representation-level and at a more fine-grained neuron-level. Our analysis reveals interesting insights, such as: i) 85% of the neurons across the network are redundant and ii) at least 92% of them can be removed when optimizing towards a downstream task. Based on our analysis, we present an efficient feature-based transfer learning procedure, which maintains 97% performance while using at-most 10% of the original neurons."
Annotating Temporal Dependency Graphs via Crowdsourcing,"['Jiarui Yao', 'Haoling Qiu', 'Bonan Min', 'Nianwen Xue']",,
AnswerFact: Fact Checking in Product Question Answering,"['Wenxuan Zhang', 'Yang Deng', 'Jing Ma', 'Wai Lam']",,
Are All Good Word Vector Spaces Isomorphic?,"['Ivan Vuliƒá', 'Sebastian Ruder', 'Anders S√∏gaard']",http://arxiv.org/abs/2004.04070v1,"Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic. As a result, they perform poorly or fail completely on non-isomorphic spaces. Such non-isomorphism has been hypothesised to result almost exclusively from typological differences between languages. In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces. We present a series of experiments across diverse languages which show that, besides inherent typological differences, variance in performance across language pairs can largely be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g. ""under-training"")."
Argument Pair Extraction from Peer Review and Rebuttal via Multi-task Learning,"['Liying Cheng', 'Lidong Bing', 'Qian Yu', 'Wei Lu', 'Luo Si']",,
Asking without Telling: Exploring Latent Ontologies in Contextual Representations,"['Julian Michael', 'Jan A. Botha', 'Ian Tenney']",http://arxiv.org/abs/2004.14513v1,"The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods."
Aspect-Based Sentiment Analysis by Aspect-Sentiment Joint Embedding,"['Jiaxin Huang', 'Yu Meng', 'Fang Guo', 'Heng Ji', 'Jiawei Han']",,
Assessing Phrasal Representation and Composition in Transformers,"['Lang Yu', 'Allyson Ettinger']",http://arxiv.org/abs/2010.03763v1,"Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models."
Assessing the Helpfulness of Learning Materials with Inference-Based Learner-Like Agent,"['Yun-Hsuan Jen', 'Chieh-Yang Huang', 'MeiHua Chen', 'Ting-Hao Huang', 'Lun-Wei Ku']",http://arxiv.org/abs/2010.02179v1,"Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ. Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways. We notice that the helpfulness of the learning material would reflect on the learners' performance. Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent's performance. To enable the agent to behave like a learner, we leverage entailment modeling's capability of inferring answers from the provided materials. Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks. We further conduct a classroom user study with college ESL learners. The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently. Compared to other models, the proposed agent improves the score of more than 17% of students after learning."
Attention Is All You Need for Chinese Word Segmentation,"['Sufeng Duan', 'Hai Zhao']",http://arxiv.org/abs/1910.14537v3,"Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model. Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer. With the effective encoder design, our model only needs to take unigram features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark datasets. The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting."
Attention is Not Only a Weight: Analyzing Transformers with Vector Norms,"['Goro Kobayashi', 'Tatsuki Kuribayashi', 'Sho Yokoi', 'Kentaro Inui']",http://arxiv.org/abs/2004.10102v2,"Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers."
AttnIO: Knowledge Graph Exploration with In-and-Out Attention Flow for Knowledge-Grounded Dialogue,"['Jaehun Jung', 'Bokyung Son', 'Sungwon Lyu']",,
Augmented Natural Language for Generative Sequence Labeling,"['Ben Athiwaratkun', 'C√≠cero Nogueira dos Santos', 'Jason Krone', 'Bing Xiang']",http://arxiv.org/abs/2009.13272v1,"We propose a generative framework for joint sequence labeling and sentence-level classification. Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space. Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks. Our framework is general purpose, performing well on few-shot, low-resource, and high-resource tasks. We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks. We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot ($75.0\% \rightarrow 90.9\%$) and 1-shot ($70.4\% \rightarrow 81.0\%$) state-of-the-art results. Furthermore, our model generates large improvements ($46.27\% \rightarrow 63.83\%$) in low-resource slot labeling over a BERT baseline by incorporating label semantics. We also maintain competitive results on high-resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset."
Authorship Attribution for Neural Text Generation,"['Adaku Uchendu', 'Thai Le', 'Kai Shu', 'Dongwon Lee']",,
Automatic Extraction of Rules Governing Morphological Agreement,"['Aditi Chaudhary', 'Antonios Anastasopoulos', 'Adithya Pratapa', 'David R. Mortensen', 'Zaid Sheikh', 'Yulia Tsvetkov', 'Graham Neubig']",http://arxiv.org/abs/2010.01160v2,"Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world's languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/."
Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing,"['Brian Thompson', 'Matt Post']",http://arxiv.org/abs/2004.14564v1,"We propose the use of a sequence-to-sequence paraphraser for automatic machine translation evaluation. The paraphraser takes a human reference as input and then force-decodes and scores an MT system output. We propose training the aforementioned paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot ""language pair"" (e.g., Russian to Russian). We denote our paraphraser ""unbiased"" because the mode of our model's output probability is centered around a copy of the input sequence, which in our case represent the best case scenario where the MT system output matches a human reference. Our method is simple and intuitive, and our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT19 segment-level shared metrics task in all languages, excluding Gujarati where the model had no training data. We also explore using our model conditioned on the source instead of the reference, and find that it outperforms every quality estimation as a metric system from the WMT19 shared task on quality estimation by a statistically significant margin in every language pair."
AutoQA: From Databases To Q&A Semantic Parsers With Only Synthetic Training Data,"['Silei Xu', 'Sina Semnani', 'Giovanni Campagna', 'Monica Lam']",,
Autoregressive Knowledge Distillation through Imitation Learning,"['Alexander Lin', 'Jeremy Wohlwend', 'Howard Chen', 'Tao Lei']",http://arxiv.org/abs/2009.07253v1,"The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures. However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings. We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation. The algorithm is designed to address the exposure bias problem. On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation. Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model."
AxCell: Automatic Extraction of Results from Machine Learning Papers,"['Marcin Kardas', 'Piotr Czapla', 'Pontus Stenetorp', 'Sebastian Ruder', 'Sebastian Riedel', 'Ross Taylor', 'Robert Stojnic']",,
Backpropagation-based Decoding for Unsupervised Counterfactual and Abductive Reasoning,"['Lianhui Qin', 'Vered Shwartz', 'Peter West', 'Chandra Bhagavatula', 'Jena D. Hwang', 'Ronan Le Bras', 'Antoine Bosselut', 'Yejin Choi']",,
Be More with Less: Hypergraph Attention Networks for Inductive Text Classification,"['Kaize Ding', 'Jianling Wang', 'Jundong Li', 'Dingcheng Li', 'Huan Liu']",,
Benchmarking Meaning Representations in Neural Semantic Parsing,"['Jiaqi Guo', 'Qian Liu', 'Jian-Guang LOU', 'Zhenwen Li', 'Xueqing Liu', 'Tao Xie', 'Ting Liu']",,
"BERT Knows Punta Cana is not just beautiful, it's gorgeous: Ranking Scalar Adjectives with Contextualised Representations","['Aina Gar√≠ Soler', 'Marianna Apidianaki']",http://arxiv.org/abs/2010.02686v1,"Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources."
BERT-ATTACK: Adversarial Attack Against BERT Using BERT,"['linyang li', 'Ruotian Ma', 'Qipeng Guo', 'Xiangyang Xue', 'Xipeng Qiu']",http://arxiv.org/abs/2004.09984v3,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose \textbf{BERT-Attack}, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack."
BERT-EMD: Many-to-Many Layer Mapping for BERT Compression with Earth Mover's Distance,"['jianquan li', 'Xiaokang Liu', 'Honghong Zhao', 'Ruifeng Xu', 'Min Yang', 'yaohong jin']",,
BERT-enhanced Relational Sentence Ordering Network,"['Baiyun Cui', 'Yingming Li', 'Zhongfei Zhang']",,
BERT-of-Theseus: Compressing BERT by Progressive Module Replacing,"['Canwen Xu', 'Wangchunshu Zhou', 'Tao Ge', 'Furu Wei', 'Ming Zhou']",http://arxiv.org/abs/2002.02925v4,"In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing. Our approach first divides the original BERT into several modules and builds their compact substitutes. Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules. We progressively increase the probability of replacement through the training. In this way, our approach brings a deeper level of interaction between the original and compact models. Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function. Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression."
Better Highlighting: Creating Sub-Sentence Summary Highlights,"['Sangwoo Cho', 'Kaiqiang Song', 'Chen Li', 'Dong Yu', 'Hassan Foroosh', 'Fei Liu']",,
Beyond Geolocation: Micro-Dialect Identification in Diaglossic and Code-Switched Environments,"['Muhammad Abdul-Mageed', 'Chiyu Zhang', 'AbdelRahim Elmadany', 'Lyle Ungar']",,
Biomedical Event Extraction as Sequence Labeling,"['Alan Ramponi', 'Rob van der Goot', 'Rosario Lombardo', 'Barbara Plank']",,
BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues,"['Hung Le', 'Doyen Sahoo', 'Nancy Chen', 'Steven C.H. Hoi']",,
Blank Language Models,"['Tianxiao Shen', 'Victor Quach', 'Regina Barzilay', 'Tommi Jaakkola']",http://arxiv.org/abs/2002.03079v1,"We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks. Unlike previous masked language models or the Insertion Transformer, BLM uses blanks to control which part of the sequence to expand. This fine-grained control of generation is ideal for a variety of text editing and rewriting tasks. The model can start from a single blank or partially completed text with blanks at specified locations. It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill. BLM can be efficiently trained using a lower bound of the marginal data likelihood, and achieves perplexity comparable to traditional left-to-right language models on the Penn Treebank and WikiText datasets. On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency. Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications."
BLEU might be Guilty but References are not Innocent,"['Markus Freitag', 'David Grangier', 'Isaac Caswell']",http://arxiv.org/abs/2004.06063v1,"The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective."
Bridging linguistic typology and multilingual machine translation with multi-view language representations,"['Arturo Oncevay', 'Barry Haddow', 'Alexandra Birch']",http://arxiv.org/abs/2004.14923v1,"Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other's language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches."
Bridging the Gap between Prior and Posterior Knowledge Selection for Knowledge-Grounded Dialogue Generation,"['Xiuyi Chen', 'Fandong Meng', 'Peng Li', 'Feilong Chen', 'Shuang Xu', 'Bo Xu', 'Jie Zhou']",,
Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing,"['Lingkai Kong', 'Haoming Jiang', 'Yuchen Zhuang', 'Jie Lyu', 'Tuo Zhao', 'Chao Zhang']",,
Can Automatic Post-Editing Improve NMT?,"['Shamil Chollampatt', 'Raymond Hendy Susanto', 'Liling Tan', 'Ewa Szymanska']",http://arxiv.org/abs/2009.14395v1,"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra."
Can Emojis Convey Human Emotions? A Study to Understand the Association between Emojis and Emotions,"['Abu Awal Md Shoeb', 'Gerard de Melo']",,
CancerEmo: A Dataset for Fine-Grained Emotion Detection,"['Tiberiu Sosea', 'Cornelia Caragea']",,
CapWAP: Captioning with a Purpose,"['Adam Fisch', 'Kenton Lee', 'Ming-Wei Chang', 'Jonathan Clark', 'Regina Barzilay']",,
Causal Inference of Script Knowledge,"['Noah Weber', 'Rachel Rudinger', 'Benjamin Van Durme']",http://arxiv.org/abs/2004.01174v1,"When does a sequence of events define an everyday scenario and how can this knowledge be induced from text? Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus. We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions. Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents"
Chapter Captor: Text Segmentation in Novels,"['Charuta Pethe', 'Allen Kim', 'Steve Skiena']",,
Character-level Representations Still Improve Semantic Parsing in the Age of BERT,"['Rik van Noord', 'Antonio Toral', 'Johan Bos']",,
CHARM: Inferring Personal Attributes from Conversations,"['Anna Tigunova', 'Andrew Yates', 'Paramita Mirza', 'Gerhard Weikum']",,
CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Labeling Using BERT,"['Akshay Smit', 'Saahil Jain', 'Pranav Rajpurkar', 'Anuj Pareek', 'Andrew Ng', 'Matthew Lungren']",http://arxiv.org/abs/2004.09167v2,"The extraction of labels from radiology text reports enables large-scale training of medical imaging models. Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts. In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations. We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation. We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays."
ChiTeSQL: A Large-Scale and Pragmatic Chinese Text-to-SQL Dataset,"['Lijie Wang', 'Ao Zhang', 'Kun Wu', 'Ke Sun', 'Zhenghua Li', 'Hua Wu', 'Min Zhang', 'Haifeng Wang']",,
ChrEn: Cherokee-English Machine Translation for Endangered Language Revitalization,"['Shiyue Zhang', 'Benjamin Frey', 'Mohit Bansal']",,
CLIRMatrix: A massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval,"['Shuo Sun', 'Kevin Duh']",,
Coarse-to-Fine Pre-training for Named Entity Recognition,"['Xue Mengge', 'Bowen Yu', 'Zhenyu Zhang', 'Tingwen Liu', 'Yue Zhang', 'Bin Wang']",,
Coarse-to-Fine Query Focused Multi-Document Summarization,"['Yumo Xu', 'Mirella Lapata']",,
CoDEx: A Comprehensive Knowledge Graph Completion Benchmark,"['Tara Safavi', 'Danai Koutra']",http://arxiv.org/abs/2009.07810v2,"We present CoDEx, a set of knowledge graph completion datasets extracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty. In terms of scope, CoDEx comprises three knowledge graphs varying in size and structure, multilingual descriptions of entities and relations, and tens of thousands of hard negative triples that are plausible but verified to be false. To characterize CoDEx, we contribute thorough empirical analyses and benchmarking experiments. First, we analyze each CoDEx dataset in terms of logical relation patterns. Next, we report baseline link prediction and triple classification results on CoDEx for five extensively tuned embedding models. Finally, we differentiate CoDEx from the popular FB15K-237 knowledge graph completion dataset by showing that CoDEx covers more diverse and interpretable content, and is a more difficult link prediction benchmark. Data, code, and pretrained models are available at https://bit.ly/2EPbrJs."
Coding Textual Inputs Boosts the Accuracy of Neural Networks,"['Abdul Rafae Khan', 'Jia Xu', 'Weiwei Sun']",,
COGS: A Compositional Generalization Challenge Based on Semantic Interpretation,"['Najoung Kim', 'Tal Linzen']",,
Cold-start Active Learning through Self-Supervised Language Modeling,"['Michelle Yuan', 'Hsuan-Tien Lin', 'Jordan Boyd-Graber']",,
Cold-start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks,"['Chengyue Jiang', 'Yinggong Zhao', 'Shanbo Chu', 'Libin Shen', 'Kewei Tu']",,
Collecting Entailment Data for Pretraining: New Protocols and Negative Results,"['Samuel R. Bowman', 'Jennimaria Palomaki', 'Livio Baldini Soares', 'Emily Pitler']",,
Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection,"['Shaolei Wang', 'Zhongyuan Wang', 'Wanxiang Che', 'Ting Liu']",,
COMETA: A Corpus for Medical Entity Linking in the Social Media,"['Marco Basaldella', 'Fangyu Liu', 'Ehsan Shareghi', 'Nigel Collier']",http://arxiv.org/abs/2010.03295v2,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman's language. Meanwhile, there is a growing need for applications that can understand the public's voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data."
Comparative Evaluation of Label Agnostic Selection Bias in Multilingual Hate Speech Datasets,"['Nedjma Ousidhoum', 'Yangqiu Song', 'Dit-Yan Yeung']",,
Competence-Level Prediction and Resume-Job_Description Matching Using Context-Aware Transformer Models,"['Changmao Li', 'Elaine Fisher', 'Rebecca Thomas', 'Steve Pittard', 'Vicki Hertzberg', 'Jinho D. Choi']",,
"Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA","['Ieva Stali≈´naitƒó', 'Ignacio Iacobacci']",http://arxiv.org/abs/2009.08257v1,"Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT."
Compositional Demographic Word Embeddings,"['Charles Welch', 'Jonathan K. Kummerfeld', 'Ver√≥nica P√©rez-Rosas', 'Rada Mihalcea']",http://arxiv.org/abs/2010.02986v1,"Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations. While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users. We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion). We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations. We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them."
Compositional Phrase Alignment and Beyond,"['Yuki Arase', ""Jun'ichi Tsujii""]",,
Compressive Summarization with Plausibility and Salience Modeling,"['Shrey Desai', 'Jiacheng Xu', 'Greg Durrett']",,
Conditional Causal Relationships between Emotions and Causes in Texts,"['Xinhong Chen', 'Qing Li', 'Jianping Wang']",,
Condolences and Empathy in Online Communities,"['Naitian Zhou', 'David Jurgens']",,
ConjNLI: Natural Language Inference Over Conjunctive Sentences,"['Swarnadeep Saha', 'Yixin Nie', 'Mohit Bansal']",,
Connecting the Dots: Event Graph Schema Induction with Path Language Modeling,"['Manling Li', 'Qi Zeng', 'Ying Lin', 'Kyunghyun Cho', 'Heng Ji', 'Jonathan May', 'Nathanael Chambers', 'Clare Voss']",,
Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,"['Sean Welleck', 'Ilia Kulikov', 'Jaedeok Kim', 'Richard Yuanzhe Pang', 'Kyunghyun Cho']",http://arxiv.org/abs/2002.02492v2,"Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency."
Constrained Iterative Labeling for Open Information Extraction,"['Keshav Kolluru', 'Vaibhav Adlakha', 'Samarth Aggarwal', 'Mausam -', 'Soumen Chakrabarti']",,
Content Planning for Neural Story Generation with Aristotelian Rescoring,"['Seraphina Goldfarb-Tarrant', 'Tuhin Chakrabarty', 'Ralph Weischedel', 'Nanyun Peng']",http://arxiv.org/abs/2009.09870v1,"Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion. We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation. We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle's Poetics. We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way."
Context-Aware Answer Extraction in Question Answering,"['Yeon Seonwoo', 'Ji-Hoon Kim', 'Jung-Woo Ha', 'Alice Oh']",,
"Continuity of Topic, Interaction, and Query: Learning to Quote in Online Conversations","['Lingzhi Wang', 'Jing Li', 'Xingshan Zeng', 'Haisong Zhang', 'Kam-Fai Wong']",,
Contrastive Distillation on Intermediate Representations for Language Model Compression,"['Siqi Sun', 'Zhe Gan', 'Yuwei Fang', 'Yu Cheng', 'Shuohang Wang', 'Jingjing Liu']",http://arxiv.org/abs/2009.14167v1,"Existing language model compression methods mostly use a simple L2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one. Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network. To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective. By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student's exploitation of rich information in teacher's hidden layers. CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods."
Controllable Meaning Representation to Text Generation: Linearization and Data Augmentation Strategies,"['Chris Kedzie', 'Kathleen McKeown']",,
Controllable Story Generation with External Knowledge Using Large-Scale Language Models,"['Peng Xu', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Raul Puri', 'Pascale Fung', 'Anima Anandkumar', 'Bryan Catanzaro']",http://arxiv.org/abs/2010.00840v1,"Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%)."
Conundrums in Entity Reference Resolution,"['Jing Lu', 'Vincent Ng']",,
Conversational Semantic Parsing,"['Armen Aghajanyan', 'Jean Maillard', 'Akshat Shrivastava', 'Keith Diedrick', 'Michael Haeger', 'Haoran Li', 'Yashar Mehdad', 'Veselin Stoyanov', 'Anuj Kumar', 'Mike Lewis', 'Sonal Gupta']",http://arxiv.org/abs/2009.13655v1,"The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover."
Conversational Semantic Parsing for Dialog State Tracking,"['Jianpeng Cheng', 'Devang Agrawal', 'H√©ctor Mart√≠nez Alonso', 'Shruti Bhargava', 'Joris Driesen', 'Federico Flego', 'Dain Kaplan', 'Dimitri Kartsaklis', 'Lin Li', 'Dhivya Piraviperumal', 'Jason D Williams', 'Hong Yu', 'Diarmuid √ì S√©aghdha', 'Anders Johannsen']",,
Convolution over Hierarchical Syntactic and Lexical Graphs for Aspect Level Sentiment Analysis,"['Mi Zhang', 'Tieyun Qian']",,
Coreferential Reasoning Learning for Language Representation,"['Deming Ye', 'Yankai Lin', 'Jiaju Du', 'Zhenghao Liu', 'Peng Li', 'Maosong Sun', 'Zhiyuan Liu']",http://arxiv.org/abs/2004.06870v2,"Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning. However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse. To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context. The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT."
Counterfactual Generator: A Weakly-Supervised Method for Named Entity Recognition,"['Xiangji Zeng', 'Yunliang Li', 'Yuchen Zhai', 'Yin Zhang']",,
Counterfactual Off-Policy Training for Neural Dialogue Generation,"['Qingfu Zhu', 'Wei-Nan Zhang', 'Ting Liu', 'William Yang Wang']",,
Cross Copy Network for Dialogue Generation,"['Changzhen Ji', 'Xin Zhou', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Conghui Zhu', 'Tiejun Zhao']",,
Cross-lingual Spoken Language Understanding with Regularized Representation Alignment,"['Zihan Liu', 'Genta Indra Winata', 'Peng Xu', 'Zhaojiang Lin', 'Pascale Fung']",http://arxiv.org/abs/2009.14510v1,"Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal. To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource. First, we regularize the representation of user utterances based on their corresponding labels. Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables. Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3\% of the target language training data, achieves comparable performance to the supervised training with all the training data."
Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings,"['Yue Wang', 'Jing Li', 'Michael Lyu', 'Irwin King']",,
Cross-Thought for Sentence Encoder Pre-training,"['Shuohang Wang', 'Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Yu Cheng', 'Jingjing Liu', 'Jing Jiang']",http://arxiv.org/abs/2010.03652v1,"In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering. Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words. Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines. Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance."
CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,"['Nikita Nangia', 'Clara Vania', 'Rasika Bhalerao', 'Samuel R. Bowman']",http://arxiv.org/abs/2010.00133v1,"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress."
CSP: Code-Switching Pre-training for Neural Machine Translation,"['Zhen Yang', 'Bojie Hu', 'ambyera han', 'shen huang', 'Qi Ju']",,
DagoBERT: Generating Derivational Morphology with a Pretrained Language Model,"['Valentin Hofmann', 'Janet Pierrehumbert', 'Hinrich Sch√ºtze']",http://arxiv.org/abs/2005.00672v2,"Can pretrained language models (PLMs) generate derivationally complex words? We present the first study investigating this question, taking BERT as the example PLM. We examine BERT's derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning. Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG). Furthermore, our experiments show that the input segmentation crucially impacts BERT's derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used."
Data and Representation for Turkish Natural Language Inference,"['Emrah Budur', 'Rƒ±za √ñz√ßelik', 'Tunga Gungor', 'Christopher Potts']",,
Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation,"['Ruibo Liu', 'Guangxuan Xu', 'Chenyan Jia', 'Weicheng Ma', 'Lili Wang', 'Soroush Vosoughi']",,
Data Rejuvenation: Exploiting Inactive Training Examples for Neural Machine Translation,"['Wenxiang Jiao', 'Xing Wang', 'Shilin He', 'Irwin King', 'Michael Lyu', 'Zhaopeng Tu']",http://arxiv.org/abs/2010.02552v1,"Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models. However, the complex patterns and potential noises in the large-scale data make training NMT models difficult. In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution. We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples. The proposed framework consists of three phases. First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities. Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward-translation. Finally, the rejuvenated examples and the active examples are combined to train the final NMT model. Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models. Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability."
Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics,"['Swabha Swayamdipta', 'Roy Schwartz', 'Nicholas Lourie', 'Yizhong Wang', 'Hannaneh Hajishirzi', 'Noah A. Smith', 'Yejin Choi']",http://arxiv.org/abs/2009.10795v1,"Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce ""Data Maps""---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs, in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ""ambiguous"" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are ""easy to learn"" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds ""hard to learn""; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."
De-biased Court‚Äôs View Generation with Causality,"['Yiquan Wu', 'Kun Kuang', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun', 'Jun Xiao', 'Yueting Zhuang', 'Luo Si', 'Fei Wu']",,
Debiasing knowledge graph embeddings,"['Joseph Fisher', 'Arpit Mittal', 'Dave Palfrey', 'Christos Christodoulopoulos']",,
Deep Attentive Stock Forecasting factoring Social Media Linguistics and Company Correlations,"['Ramit Sawhney', 'Shivam Agarwal', 'Arnav Wadhwa', 'Rajiv Ratn Shah']",,
Deep Weighted MaxSAT for Aspect-based Opinion Extraction,"['Meixi Wu', 'Wenya Wang', 'Sinno Jialin Pan']",,
Dense Passage Retrieval for Open-Domain Question Answering,"['Vladimir Karpukhin', 'Barlas Oguz', 'Sewon Min', 'Patrick Lewis', 'Ledell Wu', 'Sergey Edunov', 'Danqi Chen', 'Wen-tau Yih']",http://arxiv.org/abs/2004.04906v3,"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
Design Challenges in Low-resource Cross-lingual Entity Linking,"['Xingyu Fu', 'Weijia Shi', 'Xiaodong Yu', 'Zian Zhao', 'Dan Roth']",http://arxiv.org/abs/2005.00692v2,"Cross-lingual Entity Linking (XEL), the problem of grounding mentions of entities in a foreign language text into an English knowledge base such as Wikipedia, has seen a lot of research in recent years, with a range of promising techniques. However, current techniques do not rise to the challenges introduced by text in low-resource languages (LRL) and, surprisingly, fail to generalize to text not taken from Wikipedia, on which they are usually trained. This paper provides a thorough analysis of low-resource XEL techniques, focusing on the key step of identifying candidate English Wikipedia titles that correspond to a given foreign language mention. Our analysis indicates that current methods are limited by their reliance on Wikipedia's interlanguage links and thus suffer when the foreign language's Wikipedia is small. We conclude that the LRL setting requires the use of outside-Wikipedia cross-lingual resources and present a simple yet effective zero-shot XEL system, QuEL, that utilizes search engines query logs. With experiments on 25 languages, QuEL~shows an average increase of 25\% in gold candidate recall and of 13\% in end-to-end linking accuracy over state-of-the-art baselines."
Detecting Attackable Sentences in Arguments,"['Yohan Jo', 'Seojin Bang', 'Emaad Manzoor', 'Eduard Hovy', 'Chris Reed']",http://arxiv.org/abs/2010.02660v1,"Finding attackable sentences in an argument is the first step toward successful refutation in argumentation. We present a first large-scale analysis of sentence attackability in online arguments. We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences. We demonstrate that a sentence's attackability is associated with many of these characteristics regarding the sentence's content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability. Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople."
Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News,"['Reuben Tan', 'Bryan Plummer', 'Kate Saenko']",http://arxiv.org/abs/2009.07698v4,"Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem. Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism. While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors. In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions. To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset composed of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset. In addition to the valuable insights gleaned from our user study experiments, we provide a relatively effective approach based on detecting visual-semantic inconsistencies, which will serve as an effective first line of defense and a useful reference for future work in defending against machine-generated disinformation."
Detecting Fine-Grained Cross-Lingual Semantic Divergences without Supervision by Learning To Rank,"['Eleftheria Briakou', 'Marine Carpuat']",http://arxiv.org/abs/2010.03662v1,"Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale. This work improves the prediction and annotation of fine-grained semantic divergences. We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity. We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales. Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences."
Dialogue Distillation: Open-domain Dialogue Augmentation Using Unpaired Data,"['Rongsheng Zhang', 'Yinhe Zheng', 'Jianzhi Shao', 'Xiao-Xi Mao', 'Yadong Xi', 'Minlie Huang']",http://arxiv.org/abs/2009.09427v1,"Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data. However, collecting large-scale dialogue data is usually time-consuming and labor-intensive. To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data. Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data. A ranking module is employed to filter out low-quality dialogues. Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data. Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines."
Dialogue Response Ranking Training with Large-Scale Human Feedback Data,"['Xiang Gao', 'Yizhe Zhang', 'Michel Galley', 'Chris Brockett', 'Bill Dolan']",http://arxiv.org/abs/2009.06978v1,"Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses. However, some human replies are more engaging than others, spawning more followup interactions. Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging. We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction. To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors. We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines. Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback. We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses. Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models."
Digital Voicing of Silent Speech,"['David Gaddy', 'Dan Klein']",http://arxiv.org/abs/2010.02960v1,"In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements."
Direct Segmentation Models for Streaming Speech Translation,"['Javier Iranzo-S√°nchez', 'Adri√† Gim√©nez Pastor', 'Joan Albert Silvestre-Cerd√†', 'Pau Baquero-Arnal', 'Jorge Civera Saiz', 'Alfons Juan']",,
Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading,"['Yifan Gao', 'Chien-Sheng Wu', 'Jingjing Li', 'Shafiq Joty', 'Steven C.H. Hoi', 'Caiming Xiong', 'Irwin King', 'Michael Lyu']",http://arxiv.org/abs/2010.01838v2,"Document interpretation and dialog understanding are the two major challenges for conversational machine reading. In this work, we propose Discern, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding for both document and dialog. Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation. Based on the learned EDU and entailment representations, we either reply to the user our final decision ""yes/no/irrelevant"" of the initial question, or generate a follow-up question to inquiry more information. Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation. Code and models are released at https://github.com/Yifan-Gao/Discern."
Discontinuous Constituent Parsing as Sequence Labeling,"['David Vilares', 'Carlos G√≥mez-Rodr√≠guez']",http://arxiv.org/abs/2010.00633v1,"This paper reduces discontinuous parsing to sequence labeling. It first shows that existing reductions for constituent parsing as labeling do not support discontinuities. Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence. Third, it studies whether such discontinuous representations are learnable. The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate."
Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays,"['Wei Song', 'Ziyao Song', 'Ruiji Fu', 'Lizhen Liu', 'Miaomiao Cheng', 'Ting Liu']",,
Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference,"['Jianguo Zhang', 'Kazuma Hashimoto', 'Wenhao Liu', 'Chien-Sheng Wu', 'Yao Wan', 'Philip Yu', 'Richard Socher', 'Caiming Xiong']",,
Discriminatively-Tuned Generative Classifiers for Robust Natural Language Inference,"['Xiaoan Ding', 'Tianyu Liu', 'Baobao Chang', 'Zhifang Sui', 'Kevin Gimpel']",http://arxiv.org/abs/2010.03760v1,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work . In particular, we find strong results with a simple unbounded modification to log loss, which we call the ""infinilog loss"". Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise."
Disentangle-based Continual Graph Representation Learning,"['Xiaoyu Kou', 'Yankai Lin', 'Shaobo Liu', 'Peng Li', 'Jie Zhou', 'Yan Zhang']",http://arxiv.org/abs/2010.02565v1,"Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data. However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data. To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge. Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human's ability to learn procedural knowledge. The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models. The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL."
Dissecting Span Identification Tasks with Performance Prediction,"['Sean Papay', 'Roman Klinger', 'Sebastian Pad√≥']",http://arxiv.org/abs/2010.02587v1,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive."
Distilling Multiple Domains for Neural Machine Translation,"['Anna Currey', 'Prashant Mathur', 'Georgiana Dinu']",,
"Diverse, Controllable, and Keyphrase-Aware: A Corpus and Method for News Multi-Headline Generation","['Dayiheng Liu', 'Yeyun Gong', 'Yu Yan', 'Jie Fu', 'Bo Shao', 'Daxin Jiang', 'Jiancheng Lv', 'Nan Duan']",http://arxiv.org/abs/2004.03875v2,"News headline generation aims to produce a short sentence to attract readers to read the news. One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines. However, most existing methods focus on the single headline generation. In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines. We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines. Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of $<$news article, headline, keyphrase$>$. Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity"
DiversiÔ¨Åed Multiple Instance Learning for Document-Level Multi-Aspect Sentiment ClassiÔ¨Åcation,"['Yunjie Ji', 'Hao Liu', 'Bolei He', 'Xinyan Xiao', 'Hua Wu', 'Yanhua Yu']",,
Do ‚ÄúUndocumented Immigrants‚Äù == ‚ÄúIllegal Aliens‚Äù? Differentiating Denotation and Connotation in Vector Space,"['Albert Webson', 'Zhizhong Chen', 'Carsten Eickhoff', 'Ellie Pavlick']",,
Do sequence-to-sequence VAEs learn global features of sentences?,"['Tom Bosc', 'Pascal Vincent']",http://arxiv.org/abs/2004.07683v1,"A longstanding goal in NLP is to compute global sentence representations. Such representations would be useful for sample-efficient semi-supervised learning and controllable text generation. To learn to represent global and local information separately, Bowman & al. (2016) proposed to train a sequence-to-sequence model with the variational auto-encoder (VAE) objective. What precisely is encoded in these latent variables expected to capture global features? We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we see that VAEs are prone to memorizing the first words and the sentence length, drastically limiting their usefulness. To alleviate this, we propose variants based on bag-of-words assumptions and language model pretraining. These variants learn latents that are more global: they are more predictive of topic or sentiment labels, and their reconstructions are more faithful to the labels of the original documents."
doc2dial: A Goal-Oriented Document-Grounded Dialogue Dataset,"['Song Feng', 'Hui Wan', 'Chulaka Gunasekara', 'Siva Patel', 'Sachindra Joshi', 'Luis Lastras']",,
Does my multimodal model learn cross-modal interactions? It‚Äôs harder to tell than you might think!,"['Jack Hessel', 'Lillian Lee']",,
Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction,"['Rujun Han', 'Yichao Zhou', 'Nanyun Peng']",http://arxiv.org/abs/2009.07373v2,"Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding. Prior systems leverage deep learning and pre-trained language models to improve the performance of the task. However, these systems often suffer from two short-comings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that are assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data. To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge. We solve the constrained inference problem via Lagrangian Relaxation and apply it on end-to-end event temporal relation extraction tasks. Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains."
Don't Read Too Much Into It: Adaptive Computation for Open-Domain Question Answering,"['Yuxiang Wu', 'Sebastian Riedel', 'Pasquale Minervini', 'Pontus Stenetorp']",,
DORB: Dynamically Optimizing Multiple Rewards with Bandits,"['Ramakanth Pasunuru', 'Han Guo', 'Mohit Bansal']",,
Double Graph Based Reasoning for Document-level Relation Extraction,"['Shuang Zeng', 'Runxin Xu', 'Baobao Chang', 'Lei Li']",http://arxiv.org/abs/2009.13752v1,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across a document. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN) featuring double graphs. GAIN first constructs a heterogeneous mention-level graph (hMG) to model complex interaction among different mentions across the document. It also constructs an entity-level graph (EG), based on which we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/DreamInvoker/GAIN ."
DualTKB: A Dual Learning Bridge between Text and Knowledge Base,"['Pierre Dognin', 'Igor Melnyk', 'Inkit Padhi', 'C√≠cero Nogueira dos Santos', 'Payel Das']",,
DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion,"['Zhen Han', 'Peng Chen', 'Yunpu Ma', 'Volker Tresp']",,
Dynamic Anticipation and Completion for Multi-Hop Reasoning over Sparse Knowledge Graph,"['Xin Lv', 'Xu Han', 'Lei Hou', 'Juanzi Li', 'Zhiyuan Liu', 'Wei Zhang', 'YICHI ZHANG', 'Hao Kong', 'Suhui Wu']",http://arxiv.org/abs/2010.01899v1,"Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion. Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning. On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths. On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult. To solve these problems, we propose a multi-hop reasoning model named DacKGR over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs. (2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs. The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines. Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR"
Dynamic Context Selection for Document-level Neural Machine Translation via Reinforcement Learning,"['Xiaomian Kang', 'Yang Zhao', 'Jiajun Zhang', 'Chengqing Zong']",,
Dynamic Data Selection and Weighting for Iterative Back-Translation,"['Zi-Yi Dou', 'Antonios Anastasopoulos', 'Graham Neubig']",http://arxiv.org/abs/2004.03672v2,"Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines."
Effective Unsupervised Domain Adaptation with Adversarially Trained Language Models,"['Thuy-Trang Vu', 'Dinh Phung', 'Gholamreza Haffari']",http://arxiv.org/abs/2010.01739v1,"Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest. Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of \emph{randomly} masked-out tokens. In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed. Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over \emph{subsets} of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements."
Efficient Meta Lifelong-Learning with Limited Memory,"['Zirui Wang', 'Sanket Vaibhav Mehta', 'Barnabas Poczos', 'Jaime Carbonell']",http://arxiv.org/abs/2010.02500v1,"Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time."
Eliciting Knowledge from Language Models Using Automatically Generated Prompts,"['Taylor Shin', 'Yasaman Razeghi', 'Robert L Logan IV', 'Eric Wallace', 'Sameer Singh']",,
Embedding Words in Non-Vector Space with Unsupervised Graph Learning,"['Maksim Riabinin', 'Sergei Popov', 'Liudmila Prokhorenkova', 'Elena Voita']",,
Enabling Cross-Lingual AMR Parsing with Transfer Learning Techniques,"['Rexhina Blloshmi', 'Rocco Tripodi', 'Roberto Navigli']",,
End-to-End Emotion-Cause Pair Extraction based on Sliding Window Multi-Label Learning,"['Zixiang Ding', 'Rui Xia', 'Jianfei Yu']",,
End-to-End Slot Alignment and Recognition for Cross-Lingual {NLU},"['Weijia Xu', 'Batool Haider', 'Saab Mansour']",,
End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems,"['Siamak Shakeri', 'C√≠cero Nogueira dos Santos', 'Henghui Zhu', 'Patrick Ng', 'Feng Nan', 'Zhiguo Wang', 'Ramesh Nallapati', 'Bing Xiang']",,
Enhancing Aspect Term Extraction with Soft Prototypes,"['Zhuang Chen', 'Tieyun Qian']",,
"Ensemble Distillation for Structured Prediction: Calibrated, Accurate, Fast‚ÄîChoose Three","['Steven Reich', 'David Mueller', 'Nicholas Andrews']",,
Entities as Experts: Sparse Memory Access with Entity Supervision,"['Thibault F√©vry', 'Livio Baldini Soares', 'Nicholas FitzGerald', 'Eunsol Choi', 'Tom Kwiatkowski']",http://arxiv.org/abs/2004.07202v2,"We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model. We introduce a new model - Entities as Experts (EAE) - that can access distinct memories of the entities mentioned in a piece of text. Unlike previous efforts to integrate entity knowledge into sequence models, EAE's entity representations are learned directly from text. We show that EAE's learned representations capture sufficient knowledge to answer TriviaQA questions such as ""Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?"", outperforming an encoder-generator Transformer model with 10x the parameters. According to the LAMA knowledge probes, EAE contains more factual knowledge than a similarly sized BERT, as well as previous approaches that integrate external sources of entity knowledge. Because EAE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EAE's performance."
Entity Enhanced BERT Pre-training for Chinese NER,"['Chen Jia', 'Yuefeng Shi', 'Qinrong Yang', 'Yue Zhang']",,
Entity Linking in 100 Languages,"['Jan A. Botha', 'Zifei Shan', 'Daniel Gillick']",,
ETC: Encoding Long and Structured Inputs in Transformers,"['Joshua Ainslie', 'Santiago Ontanon', 'Chris Alberti', 'Vaclav Cvicek', 'Zachary Fisher', 'Philip Pham', 'Anirudh Ravula', 'Sumit Sanghai', 'Qifan Wang', 'Li Yang']",http://arxiv.org/abs/2004.08483v4,"Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs."
Evaluating and Characterizing Human Rationales,"['Samuel Carton', 'Anirudh Rathore', 'Chenhao Tan']",,
Evaluating the Calibration of Knowledge Graph Embeddings for Trustworthy Link Prediction,"['Tara Safavi', 'Danai Koutra', 'Edgar Meij']",http://arxiv.org/abs/2004.01168v3,"Little is known about the trustworthiness of predictions made by knowledge graph embedding (KGE) models. In this paper we take initial steps toward this direction by investigating the calibration of KGE models, or the extent to which they output confidence scores that reflect the expected correctness of predicted knowledge graph triples. We first conduct an evaluation under the standard closed-world assumption (CWA), in which predicted triples not already in the knowledge graph are considered false, and show that existing calibration techniques are effective for KGE under this common but narrow assumption. Next, we introduce the more realistic but challenging open-world assumption (OWA), in which unobserved predictions are not considered true or false until ground-truth labels are obtained. Here, we show that existing calibration techniques are much less effective under the OWA than the CWA, and provide explanations for this discrepancy. Finally, to motivate the utility of calibration for KGE from a practitioner's perspective, we conduct a unique case study of human-AI collaboration, showing that calibrated predictions can improve human performance in a knowledge graph completion task."
Evaluating the Factual Consistency of Abstractive Text Summarization,"['Wojciech Kryscinski', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",,
Event Extraction as Machine Reading Comprehension,"['Jian Liu', 'Yubo Chen', 'Kang Liu', 'Wei Bi', 'Xiaojiang Liu']",,
Event Extraction by Answering (Almost) Natural Questions,"['Xinya Du', 'Claire Cardie']",http://arxiv.org/abs/2004.13625v1,"The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task, which extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (zero-shot learning setting)."
EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering,"['Momchil Hardalov', 'Todor Mihaylov', 'Dimitrina Zlatkova', 'Yoan Dinkov', 'Ivan Koychev', 'Preslav Nakov']",,
Experience Grounds Language,"['Yonatan Bisk', 'Ari Holtzman', 'Jesse Thomason', 'Jacob Andreas', 'Yoshua Bengio', 'Joyce Chai', 'Mirella Lapata', 'Angeliki Lazaridou', 'Jonathan May', 'Aleksandr Nisnevich', 'Nicolas Pinto', 'Joseph Turian']",http://arxiv.org/abs/2004.10151v2,"Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication."
Explainable Automated Fact-Checking for Public Health Claims,"['Neema Kotonya', 'Francesca Toni']",,
Explainable Clinical Decision Support from Text,"['Jinyue Feng', 'Chantal Shaib', 'Frank Rudzicz']",,
Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning,"['Tao Shen', 'Yi Mao', 'Pengcheng He', 'Guodong Long', 'Adam Trischler', 'Weizhu Chen']",http://arxiv.org/abs/2004.14224v1,"In this work, we aim at equipping pre-trained language models with structured knowledge. We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs. Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text. This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions. In addition we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective which is optimized jointly with masked language model. In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text. It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples. Experiments show that our proposed model achieves improved performance on five benchmark datasets, including question answering and knowledge base completion tasks."
"Exploring and Evaluating Attributes, Values, and Structure for Entity Alignment","['Zhiyuan Liu', 'Yixin Cao', 'Liangming Pan', 'Juanzi Li', 'Zhiyuan Liu', 'Tat-Seng Chua']",http://arxiv.org/abs/2010.03249v1,"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performances by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements ($5.10\%$ on average Hits@$1$ in DBP$15$k) over $12$ baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at https://github.com/thunlp/explore-and-evaluate."
Exploring and Predicting Transferability across NLP Tasks,"['Tu Vu', 'Tong Wang', 'Tsendsuren Munkhdalai', 'Alessandro Sordoni', 'Adam Trischler', 'Andrew Mattarella-Micke', 'Subhransu Maji', 'Mohit Iyyer']",http://arxiv.org/abs/2005.00770v2,"Recent advances in NLP demonstrate the effectiveness of training large-scale language models and transferring them to downstream tasks. Can fine-tuning these models on tasks other than language modeling further improve performance? In this paper, we conduct an extensive study of the transferability between 33 NLP tasks across three broad classes of problems (text classification, question answering, and sequence labeling). Our results show that transfer learning is more beneficial than previously thought, especially when target task data is scarce, and can improve performance even when the source task is small or differs substantially from the target task (e.g., part-of-speech tagging transfers well to the DROP QA dataset). We also develop task embeddings that can be used to predict the most transferable source tasks for a given target task, and we validate their effectiveness in experiments controlled for source and target data size. Overall, our experiments reveal that factors such as source data size, task and domain similarity, and task complexity all play a role in determining transferability."
Exploring Semantic Capacity of Terms,"['Jie Huang', 'Zilong Wang', 'Kevin Chang', 'Wen-mei Hwu', 'JinJun Xiong']",http://arxiv.org/abs/2010.01898v1,"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations."
Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation,"['Francisco Vargas', 'Ryan Cotterell']",http://arxiv.org/abs/2009.09435v2,"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias sub-space is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016)."
Extracting Implicitly Asserted Propositions in Argumentation,"['Yohan Jo', 'Jacky Visser', 'Chris Reed', 'Eduard Hovy']",http://arxiv.org/abs/2010.02654v1,"Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives. These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly. However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation. In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation. By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models. Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation."
F^2-Softmax: Diversifying Neural Text Generation via Frequency Factorized Softmax,"['Byung-Ju Choi', 'Jimin Hong', 'David Park', 'Sang Wan Lee']",http://arxiv.org/abs/2009.09417v2,"Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive. We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective. As a simple yet effective remedy, we propose two novel methods, F^2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution. MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes. F^2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (i) frequency class, and (ii) token from the target frequency class. Models learn more uniform probability distributions because they are confined to subsets of vocabularies. Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts."
F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering,"['Hendrik Schuff', 'Heike Adel', 'Ngoc Thang Vu']",,
Facilitating the Communication of Politeness through Fine-Grained Paraphrasing,"['Liye Fu', 'Susan Fussell', 'Cristian Danescu-Niculescu-Mizil']",,
Fact or Fiction: Verifying Scientific Claims,"['David Wadden', 'Shanchuan Lin', 'Kyle Lo', 'Lucy Lu Wang', 'Madeleine van Zuylen', 'Arman Cohan', 'Hannaneh Hajishirzi']",http://arxiv.org/abs/2004.14974v6,"We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org."
Fast semantic parsing with well-typedness guarantees,"['Matthias Lindemann', 'Jonas Groschwitz', 'Alexander Koller']",http://arxiv.org/abs/2009.07365v2,"AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks. It relies on a type system that models semantic valency but makes existing parsers slow. We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy."
Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training,"['Hai Ye', 'Qingyu Tan', 'Ruidan He', 'Juntao Li', 'Hwee Tou Ng', 'Lidong Bing']",http://arxiv.org/abs/2009.11538v2,"Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently. Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning. We explore unsupervised domain adaptation (UDA) in this paper. With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain. Self-training is widely used for UDA which predicts pseudo labels on the target domain data for training. However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model. To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered. We further extend CFd to a cross-language setting, in which language discrepancy is studied. Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings."
FedED: Federated Learning via Ensemble Distillation for Medical Relation Extraction,"['Dianbo Sui', 'Yubo Chen', 'Jun Zhao', 'Yantao Jia', 'Yuantao Xie', 'Weijian Sun']",,
Few-shot Complex Knowledge Base Question Answering via Meta Reinforcement Learning,"['Yuncheng Hua', 'Yuan-Fang Li', 'Gholamreza Haffari', 'Guilin Qi', 'Tongtong Wu']",,
Few-Shot Learning for Opinion Summarization,"['Arthur Bra≈æinskas', 'Mirella Lapata', 'Ivan Titov']",,
Filtering Noisy Dialogue Corpora by Connectivity and Content Relatedness,"['Reina Akama', 'Sho Yokoi', 'Jun Suzuki', 'Kentaro Inui']",http://arxiv.org/abs/2004.14008v2,"Large-scale dialogue datasets have recently become available for training neural dialogue agents. However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs. In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness. The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities. We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality. Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality. We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation."
Form2Seq : A Framework for Higher-Order Form Structure Extraction,"['Milan Aggarwal', 'Hiresh Gupta', 'Mausoom Sarkar', 'Balaji Krishnamurthy']",,
Friendly Topic Assistant for Transformer Based Abstractive Summarization,"['Zhengjue Wang', 'Zhibin Duan', 'Hao Zhang', 'chaojie wang', 'long tian', 'Bo Chen', 'Mingyuan Zhou']",,
From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers,"['Anne Lauscher', 'Vinit Ravishankar', 'Ivan Vuliƒá', 'Goran Glava≈°']",http://arxiv.org/abs/2005.00633v1,"Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions."
Frustratingly Simple Few-Shot Named Entity Recognition with Structured Nearest Neighbor Learning,"['Yi Yang', 'Arzoo Katiyar']",,
Generating Dialogue Responses from a Semantic Latent Space,"['Wei-Jen Ko', 'Avik Ray', 'Yilin Shen', 'Hongxia Jin']",http://arxiv.org/abs/2010.01658v1,"Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary. However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt. In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses. To address this issue, we propose an alternative to the end-to-end classification on vocabulary. We learn the pair relationship between the prompts and responses as a regression task on a latent space instead. In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space. Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative."
Generating Diverse Translation from Model Distribution with Dropout,"['Xuanfu Wu', 'Yang Feng', 'Chenze Shao']",,
Generating Fact Checking Briefs,"['Angela Fan', 'Aleksandra Piktus', 'Fabio Petroni', 'Guillaume Wenzek', 'Marzieh Saeidi', 'Andreas Vlachos', 'Antoine Bordes', 'Sebastian Riedel']",,
Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze,"['Ece Takmaz', 'Sandro Pezzelle', 'Lisa Beinborn', 'Raquel Fern√°ndez']",,
Generating Radiology Reports via Memory-driven Transformer,"['Zhihong Chen', 'Yan Song', 'Tsung-Hui Chang', 'Xiang Wan']",,
Generating similes eÃ∂fÃ∂fÃ∂oÃ∂rÃ∂tÃ∂lÃ∂eÃ∂sÃ∂sÃ∂lÃ∂yÃ∂ ùò≠ùò™ùò¨ùò¶ ùò¢ ùòóùò≥ùò∞: A Style Transfer Approach for Simile Generation,"['Tuhin Chakrabarty', 'Smaranda Muresan', 'Nanyun Peng']",,
"Generationary or: ""How we Went Beyond Word Sense Inventories and Learned to Gloss""","['Michele Bevilacqua', 'Marco Maru', 'Roberto Navigli']",,
Global-to-Local Neural Networks for Document-Level Relation Extraction,"['Difeng Wang', 'Wei Hu', 'Ermei Cao', 'Weijian Sun']",http://arxiv.org/abs/2009.10359v1,"Relation extraction (RE) aims to identify the semantic relations between named entities in text. Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document. In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations. Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations. Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE. It is particularly effective in extracting relations between entities of long distance and having multiple mentions."
GLUCOSE: GeneraLized and COntextualized Story Explanations,"['Nasrin Mostafazadeh', 'Aditya Kalyanpur', 'Lori Moon', 'David Buchanan', 'Lauren Berkowitz', 'Or Biran', 'Jennifer Chu-Carroll']",http://arxiv.org/abs/2009.07758v1,"When humans read or listen, they make implicit commonsense inferences that frame their understanding of what happened and why. As a step toward AI systems that can build similar mental models, we introduce GLUCOSE, a large-scale dataset of implicit commonsense causal knowledge, encoded as causal mini-theories about the world, each grounded in a narrative context. To construct GLUCOSE, we drew on cognitive psychology to identify ten dimensions of causal explanation, focusing on events, states, motivations, and emotions. Each GLUCOSE entry includes a story-specific causal statement paired with an inference rule generalized from the statement. This paper details two concrete contributions: First, we present our platform for effectively crowdsourcing GLUCOSE data at scale, which uses semi-structured templates to elicit causal explanations. Using this platform, we collected 440K specific statements and general rules that capture implicit commonsense knowledge about everyday situations. Second, we show that existing knowledge resources and pretrained language models do not include or readily predict GLUCOSE's rich inferential content. However, when state-of-the-art neural models are trained on this knowledge, they can start to make commonsense inferences on unseen stories that match humans' mental models."
Gone At Last: Removing the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training,"['Joe Stacey', 'Pasquale Minervini', 'Haim Dubossarsky', 'Sebastian Riedel', 'Tim Rockt√§schel']",,
GRADE: Automatic Graph-Enhanced Coherence Metric for Evaluating Open-Domain Dialogue Systems,"['Lishan Huang', 'Zheng Ye', 'Jinghui Qin', 'Liang Lin', 'Xiaodan Liang']",http://arxiv.org/abs/2010.03994v1,"Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems. However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows. Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics. Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation. Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence. The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights. Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgements. Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics."
Gradient-guided Unsupervised Lexically Constrained Text Generation,['Lei Sha'],,
Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses,"['Simon Flachs', 'Oph√©lie Lacroix', 'Helen Yannakoudakis', 'Marek Rei', 'Anders S√∏gaard']",,
Graph Convolutions over Constituent Trees for Syntax-Aware Semantic Role Labeling,"['Diego Marcheggiani', 'Ivan Titov']",http://arxiv.org/abs/1909.09814v2,"Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles. Even though most semantic-role formalisms are built upon constituent syntax and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax. In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system. Nodes in our SpanGCN correspond to constituents. The computation is done in 3 stages. First, initial node representations are produced by `composing' word representations of the first and the last word in the constituent. Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations. Finally, the constituent representations are `decomposed' back into word representations which in turn are used as input to the SRL classifier. We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard CoNLL-2005, CoNLL-2012, and FrameNet benchmarks."
GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems,"['Shiquan Yang', 'Rui Zhang', 'Sarah Erfani']",http://arxiv.org/abs/2010.01447v1,"End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs. There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs. To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure. Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets."
Grounded Adaptation for Zero-shot Executable Semantic Parsing,"['Victor Zhong', 'Mike Lewis', 'Sida I. Wang', 'Luke Zettlemoyer']",http://arxiv.org/abs/2009.07396v2,"We propose Grounded Adaptation for Zero-shot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas). GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g. utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser. Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified. On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser. Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation."
Grounded Compositional Outputs for Adaptive Language Modeling,"['Nikolaos Pappas', 'Phoebe Mulcaire', 'Noah A. Smith']",http://arxiv.org/abs/2009.11523v2,"Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks. A language model's vocabulary$-$typically selected before training and permanently fixed later$-$affects its size and is part of what makes it resistant to such adaptation. Prior work has used compositional input embeddings based on surface forms to ameliorate this issue. In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions. To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words."
H2KGAT: Hierarchical Hyperbolic Knowledge Graph Attention Network,"['Shen Wang', 'Xiaokai Wei', 'C√≠cero Nogueira dos Santos', 'Zhiguo Wang', 'Ramesh Nallapati', 'Andrew Arnold', 'Bing Xiang', 'Philip S. Yu']",,
HABERTOR: An Efficient and Effective Deep Hatespeech Detector,"['Thanh Tran', 'Yifan Hu', 'Changwei Hu', 'Kevin Yen', 'Fei Tan', 'Kyumin Lee', 'Se Rim Park']",,
Hate-Speech and Offensive Language Detection in Roman Urdu,"['Hammad Rizwan', 'Muhammad Haroon Shakeel', 'Asim Karim']",,
Help! Need Advice on Identifying Advice,"['Venkata Subrahmanyan Govindarajan', 'Benjamin Chen', 'Rebecca Warholic', 'Katrin Erk', 'Junyi Jessy Li']",,
HENIN: Learning Heterogeneous Neural Interaction Networks for Explainable Cyberbullying Detection on Social Media,"['Chen Hsin-Yu', 'Cheng-Te Li']",,
Hero: Hierarchical Encoder for Video+Language Omni-representation Pre-training,"['Linjie Li', 'Yen-Chun Chen', 'Yu Cheng', 'Zhe Gan', 'Licheng Yu', 'Jingjing Liu']",http://arxiv.org/abs/2005.00200v2,"We present HERO, a novel framework for large-scale video+language omni-representation learning. HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer. In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames. HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions. Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains. We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities."
Hierarchical Evidence Set Modeling for Automated Fact Extraction and Verification,"['Shyam Subramanian', 'Kyumin Lee']",,
Hierarchical Graph Network for Multi-hop Question Answering,"['Yuwei Fang', 'Siqi Sun', 'Zhe Gan', 'Rohit Pillai', 'Shuohang Wang', 'Jingjing Liu']",http://arxiv.org/abs/1911.03631v4,"In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches."
HIT: Nested Named Entity Recognition via Head-Tail Pair and Token Interaction,"['Yu Wang', 'Yun Li', 'Hanghang Tong', 'Ziye Zhu']",,
How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking,"['Nicola De Cao', 'Michael Sejr Schlichtkrull', 'Wilker Aziz', 'Ivan Titov']",http://arxiv.org/abs/2004.14992v2,"Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering."
Human-centric dialog training via offline reinforcement learning,"['Natasha Jaques', 'Judy Hanwen Shen', 'Asma Ghandeharioun', 'Craig Ferguson', 'Agata Lapedriza', 'Noah Jones', 'Shixiang Gu', 'Rosalind Picard']",,
Human-in-the-loop Debugging Deep Text Classifiers,"['Piyawat Lertvittayakumjorn', 'Lucia Specia', 'Francesca Toni']",,
STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,"['Nader Akoury', 'Shufan Wang', 'Josh Whiting', 'Stephen Hood', 'Nanyun Peng', 'Mohit Iyyer']",http://arxiv.org/abs/2010.01717v1,"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation."
Identifying Elements Essential for BERT‚Äôs Multilinguality,"['Philipp Dufter', 'Hinrich Sch√ºtze']",,
I‚Äôd rather just go to bed: Understanding Indirect Answers,"['Annie Louis', 'Dan Roth', 'Filip Radlinski']",http://arxiv.org/abs/2010.03450v1,"We revisit a pragmatic inference problem in dialog: understanding indirect responses to questions. Humans can interpret 'I'm starving.' in response to 'Hungry?', even without direct cue words such as 'yes' and 'no'. In dialog systems, allowing natural responses rather than closed vocabularies would be similarly beneficial. However, today's systems are only as sensitive to these pragmatic moves as their language model allows. We create and release the first large-scale English language corpus 'Circa' with 34,268 (polar question, indirect answer) pairs to enable progress on this task. The data was collected via elaborate crowdsourcing, and contains utterances with yes/no meaning, as well as uncertain, middle-ground, and conditional responses. We also present BERT-based neural models to predict such categories for a question-answer pair. We find that while transfer learning from entailment works reasonably, performance is not yet sufficient for robust dialog. Our models reach 82-88% accuracy for a 4-class distinction, and 74-85% for 6 classes."
"If beam search is the answer, what was the question?","['Clara Meister', 'Ryan Cotterell', 'Tim Vieira']",http://arxiv.org/abs/2010.02650v1,"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU."
IGSQL: Database Schema Interaction Graph Based Neural Model for Context-Dependent Text-to-SQL Generation,"['Yitao Cai', 'Xiaojun Wan']",,
IGT2P: From Interlinear Glossed Texts to Paradigms,"['Sarah Moeller', 'Ling Liu', 'Changbing Yang', 'Katharina Kann', 'Mans Hulden']",,
IIRC: A Dataset of Incomplete Information Reading Comprehension Questions,"['James Ferguson', 'Matt Gardner', 'Hannaneh Hajishirzi', 'Tushar Khot', 'Pradeep Dasigi']",,
Imitation Attacks and Defenses for Black-box Machine Translation Systems,"['Eric Wallace', 'Mitchell Stern', 'Dawn Song']",http://arxiv.org/abs/2004.15015v2,"Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed."
Improving AMR parsing with Sequence-to-Sequence Pre-training,"['Dongqin Xu', 'Junhui Li', 'Muhua Zhu', 'Min Zhang', 'Guodong Zhou']",http://arxiv.org/abs/2010.01771v1,"In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https://github.com/xdqkid/S2S-AMR-Parser."
Improving Detection and Categorization of Task-relevant Utterances through Integration of Discourse Structure and Ontological Knowledge,"['Sopan Khosla', 'Shikhar Vashishth', 'Jill Fain Lehman', 'Carolyn Rose']",,
Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples,"['Lihao Wang', 'Xiaoqing Zheng']",,
Improving Neural Topic Models using Knowledge Distillation,"['Alexander Miserlis Hoyle', 'Pranav Goel', 'Philip Resnik']",,
Improving Out-of-Scope Detection in Intent Classification by Using Embeddings of the Word Graph Space of the Classes,"['Paulo Cavalin', 'Victor Henrique Alves Ribeiro', 'Ana Appel', 'Claudio Pinhanez']",,
Improving Text Generation with Student-Forcing Optimal Transport,"['Jianqiao Li', 'Chunyuan Li', 'Guoyin Wang', 'Hao Fu', 'Yuhchen Lin', 'Liqun Chen', 'Yizhe Zhang', 'Chenyang Tao', 'Ruiyi Zhang', 'Wenlin Wang', 'Dinghan Shen', 'Qian Yang', 'Lawrence Carin']",,
Improving Word Sense Disambiguation with Translations,"['Yixing Luan', 'Bradley Hauer', 'Lili Mou', 'Grzegorz Kondrak']",,
Incomplete Utterance Rewriting as Semantic Segmentation,"['Qian Liu', 'Bei Chen', 'Jian-Guang LOU', 'Bin Zhou', 'Dongmei Zhang']",http://arxiv.org/abs/2009.13166v1,"Recent years the task of incomplete utterance rewriting has raised a large attention. Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism. In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task. Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix. Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets. Furthermore, our approach is four times faster than the standard approach in inference."
Incorporating Multimodal Information in Open-Domain Web Keyphrase Extraction,"['Yansen Wang', 'Zhen Fan', 'Carolyn Rose']",,
Incremental Event Detection via Knowledge Consolidation Networks,"['Pengfei Cao', 'Yubo Chen', 'Jun Zhao', 'Taifeng Wang']",,
Incremental Processing in the Age of Non-Incremental Encoders: An Empirical Assessment of Bidirectional Models for Incremental NLU,"['Brielen Madureira', 'David Schlangen']",,
Inducing Target-specific Latent Structures for Aspect Sentiment Classification,"['Chenhua Chen', 'Zhiyang Teng', 'Yue Zhang']",,
Information Seeking in the Spirit of Learning: a Dataset for Conversational Curiosity,"['Pedro Rodriguez', 'Paul Crook', 'Seungwhan Moon', 'Zhiguang Wang']",http://arxiv.org/abs/2005.00172v1,"Open-ended human learning and information-seeking are increasingly mediated by technologies like digital assistants. However, such systems often fail to account for the user's pre-existing knowledge, which is a powerful way to increase engagement and to improve retention. Assuming a correlation between engagement and user responses such as ""liking"" messages or asking followup questions, we design a Wizard of Oz dialog task that tests the hypothesis that engagement increases when users are presented with facts that relate to their existing knowledge. Through crowd-sourcing of this experimental task we collected and now open-source 14K dialogs (181K utterances) where users and assistants converse about various aspects related to geographic entities. This dataset is annotated with pre-existing user knowledge, message-level dialog acts, message grounding to Wikipedia, user reactions to messages, and per-dialog ratings. Our analysis shows that responses which incorporate a user's prior knowledge do increase engagement. We incorporate this knowledge into a state-of-the-art multi-task model that reproduces human assistant policies, improving over content selection baselines by 13 points."
Information-Theoretic Probing with Minimum Description Length,"['Elena Voita', 'Ivan Titov']",http://arxiv.org/abs/2003.12298v1,"To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates ""the amount of effort"" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes."
"Infusing Disease Knowledge into BERT for Health Question Answering, Medical Inference and Disease Name Recognition","['Yun He', 'Ziwei Zhu', 'Yin Zhang', 'Qin Chen', 'James Caverlee']",http://arxiv.org/abs/2010.03746v1,"Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment. This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition. While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects. Hence, we integrate BERT with disease knowledge for improving these important tasks. Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT. Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion. For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets. We make our data and code freely available."
Inquisitive Question Generation for High Level Text Comprehension,"['Wei-Jen Ko', 'TE-YUAN CHEN', 'Yiyan Huang', 'Greg Durrett', 'Junyi Jessy Li']",http://arxiv.org/abs/2010.01657v1,"Inquisitive probing questions come naturally to humans in a variety of settings, but is a challenging task for automatic systems. One natural type of question to ask tries to fill a gap in knowledge during text comprehension, like reading a news article: we might ask about background information, deeper reasons behind things occurring, or more. Despite recent progress with data-driven approaches, generating such questions is beyond the range of models trained on existing datasets. We introduce INQUISITIVE, a dataset of ~19K questions that are elicited while a person is reading through a document. Compared to existing datasets, INQUISITIVE questions target more towards high-level (semantic and discourse) comprehension of text. We show that readers engage in a series of pragmatic strategies to seek information. Finally, we evaluate question generation models based on GPT-2 and show that our model is able to generate reasonable questions although the task is challenging, and highlight the importance of context to generate INQUISITIVE questions."
INSPIRED: Toward Sociable Recommendation Dialog Systems,"['Shirley Anugrah Hayati', 'Dongyeop Kang', 'Qingxiaoyang Zhu', 'Weiyan Shi', 'Zhou Yu']",http://arxiv.org/abs/2009.14306v2,"In recommendation dialogs, humans commonly disclose their preference and make recommendations in a friendly manner. However, this is a challenge when developing a sociable recommendation dialog system, due to the lack of dialog dataset annotated with such sociable strategies. Therefore, we present INSPIRED, a new dataset of 1,001 human-human dialogs for movie recommendation with measures for successful recommendations. To better understand how humans make recommendations in communication, we design an annotation scheme related to recommendation strategies based on social science theories and annotate these dialogs. Our analysis shows that sociable recommendation strategies, such as sharing personal opinions or communicating with encouragement, more frequently lead to successful recommendations. Based on our dataset, we train end-to-end recommendation dialog systems with and without our strategy labels. In both automatic and human evaluation, our model with strategy incorporation outperforms the baseline model. This work is a first step for building sociable recommendation dialog systems with a basis of social science theories."
Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,"['Xiaoxiao Guo', 'Mo Yu', 'Yupeng Gao', 'Chuang Gan', 'Murray Campbell', 'Shiyu Chang']",http://arxiv.org/abs/2010.02386v1,"Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. Our source code is available at: https://github.com/XiaoxiaoGuo/rcdqn."
Interactive Refinement of Cross-Lingual Word Embeddings,"['Michelle Yuan', 'Mozhi Zhang', 'Benjamin Van Durme', 'Leah Findlater', 'Jordan Boyd-Graber']",http://arxiv.org/abs/1911.03070v3,"Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages. We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem. First, CLIME ranks words by their salience to the downstream task. Then, users mark similarity between keywords and their nearest neighbors in the embedding space. Finally, CLIME updates the embeddings using the annotations. We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings. CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results."
Interpretable Multi-dataset Evaluation for Named Entity Recognition,"['Jinlan Fu', 'Pengfei Liu', 'Graham Neubig']",,
Interpretation of NLP models through input marginalization,"['Siwon Kim', 'Jihun Yi', 'Eunji Kim', 'Sungroh Yoon']",,
Interpreting Open-Domain Modifiers: Decomposition of Wikipedia Categories into Disambiguated Property-Value Pairs,['Marius Pasca'],,
Interview: Large-scale Modeling of Media Dialog with Discourse Patterns and Knowledge Grounding,"['Bodhisattwa Prasad Majumder', 'Shuyang Li', 'Jianmo Ni', 'Julian McAuley']",,
Intrinsic Evaluation of Summarization Datasets,"['Rishi Bommasani', 'Claire Cardie']",,
Intrinsic Probing through Dimension Selection,"['Lucas Torroba Hennigen', 'Adina Williams', 'Ryan Cotterell']",http://arxiv.org/abs/2010.02812v1,"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT."
Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,"['Amir Pouran Ben Veyseh', 'Nasim Nouri', 'Franck Dernoncourt', 'Dejing Dou', 'Thien Huu Nguyen']",,
Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model,"['Jun Yen Leung', 'Guy Emerson', 'Ryan Cotterell']",,
Investigating Lexical Variability in Language Models,"['Charles Yu', 'Ryan Sie', 'Nicolas Tedeschi', 'Leon Bergen']",,
Is Chinese Word Segmentation a Solved Task? Rethinking Neural Chinese Word Segmentation,"['Jinlan Fu', 'Pengfei Liu', 'Qi Zhang', 'Xuanjing Huang']",,
Is Multihop QA in DiRe Condition? Measuring and Reducing Disconnected Reasoning,"['Harsh Trivedi', 'Niranjan Balasubramanian', 'Tushar Khot', 'Ashish Sabharwal']",http://arxiv.org/abs/2005.00789v2,"Has there been real progress in multi-hop question-answering? Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts. This limits our ability to measure true progress and defeats the purpose of building multihop QA datasets. We make three contributions towards addressing this. First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts. This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning. Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning. Third, our experiments demonstrate that there hasn't been much progress in multifact reasoning. For a recent large-scale model (XLNet), we show that only 18% of its answer score is obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline. Our transformation shows a substantial reduction in disconnected reasoning (nearly 19 points in answer F1). It is complementary to adversarial approaches, yielding further reductions in conjunction."
ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention,"['Jose Manuel Gomez-Perez', 'Ra√∫l Ortega']",,
Iterative Domain-Repaired Back-Translation,"['Hao-Ran Wei', 'Zhirui Zhang', 'Boxing Chen', 'Weihua Luo']",,
Iterative Language-Based Image Editing via Self-Supervised Counterfactual Reasoning,"['Tsu-Jui Fu', 'Xin Wang', 'Scott Grafton', 'Miguel Eckstein', 'William Yang Wang']",http://arxiv.org/abs/2009.09566v2,"Iterative Language-Based Image Editing (IL-BIE) tasks follow iterative instructions to edit images step by step. Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes. However, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair. Such ability results from counterfactual thinking and the ability to think about alternatives to events that have happened already. In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity. SSCR allows the model to consider out-of-distribution instructions paired with previous images. With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario. Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw). Even with only 50% of the training data, SSCR achieves a comparable result to using complete data."
Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation,"['Jason Lee', 'Raphael Shu', 'Kyunghyun Cho']",http://arxiv.org/abs/2009.07177v1,"We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space. Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using only the latent variable as input. This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability. As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space. We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables. We evaluate our approach on WMT'14 En-De, WMT'16 Ro-En and IWSLT'16 De-En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps. On WMT'14 En-De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU)."
Joint Constrained Learning for Event-Event Relation Extraction,"['Haoyu Wang', 'Muhao Chen', 'Hongming Zhang', 'Dan Roth']",,
Joint Estimation and Analysis of Risk Behavior Ratings in Movie Scripts,"['Victor Martinez', 'Krishna Somandepalli', 'Yalda Tehranian-Uhls', 'Shrikanth Narayanan']",,
Keep CALM and Explore: Language Models for Action Generation in Text-based Games,"['Shunyu Yao', 'Rohan Rao', 'Matthew Hausknecht', 'Karthik Narasimhan']",http://arxiv.org/abs/2010.02903v1,"Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces. In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state. Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history. We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards. We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training. Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model. Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions. Code and data are available at https://github.com/princeton-nlp/calm-textgame."
Keeping Up Appearances: Computational Modeling of Face Acts in Persuasion Oriented Discussions,"['Ritam Dutt', 'Rishabh Joshi', 'Carolyn Rose']",http://arxiv.org/abs/2009.10815v2,"The notion of face refers to the public self-image of an individual that emerges both from the individual's own actions as well as from the interaction with others. Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction. Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models. The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations. Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success). Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings."
KERMIT: Complementing Transformer Architectures with Encoders of Explicit Syntactic Interpretations,"['Fabio Massimo Zanzotto', 'Andrea Santilli', 'Leonardo Ranaldi', 'Dario Onorati', 'Pierfrancesco Tommasino', 'Francesca Fallucchi']",,
KGLM: Pretrained Knowledge-Grounded Language Model for Data-to-Text Generation,"['Wenhu Chen', 'Yu Su', 'Xifeng Yan', 'William Yang Wang']",,
Knowledge Association with Hyperbolic Knowledge Graph Embeddings,"['Zequn Sun', 'Muhao Chen', 'Wei Hu', 'Chengming Wang', 'Jian Dai', 'Wei Zhang']",http://arxiv.org/abs/2010.02162v1,"Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations. Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs. They also depend on high embedding dimensions to realize enough expressiveness. Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association. We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation. Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method."
Knowledge Graph Alignment with Entity-Pair Embedding,"['Zhichun Wang', 'Jinjian Yang', 'Xiaoju Ye']",,
Knowledge Graph Empowered Entity Description Generation,"['Liying Cheng', 'Dekun Wu', 'Lidong Bing', 'Yan Zhang', 'Zhanming Jie', 'Wei Lu', 'Luo Si']",http://arxiv.org/abs/2004.14813v1,"Existing works on KG-to-text generation take as input a few RDF triples or key-value pairs conveying the knowledge of some entities to generate a natural language description. Existing datasets, such as WikiBIO, WebNLG, and E2E, basically have a good alignment between an input triple/pair set and its output text. However in practice, the input knowledge could be more than enough, because the output description may only want to cover the most significant knowledge. In this paper, we introduce a large-scale and challenging dataset to facilitate the study of such practical scenario in KG-to-text. Our dataset involves exploring large knowledge graphs (KG) to retrieve abundant knowledge of various types of main entities, which makes the current graph-to-sequence models severely suffered from the problems of information loss and parameter explosion while generating the description text. We address these challenges by proposing a multi-graph structure that is able to represent the original graph information more comprehensively. Furthermore, we also incorporate aggregation methods that learn to ensemble the rich graph information. Extensive experiments demonstrate the effectiveness of our model architecture."
Knowledge-Grounded Dialogue Generation with Pre-trained Language Models,"['Xueliang Zhao', 'wei wu', 'Can Xu', 'Chongyang Tao', 'Dongyan Zhao', 'Rui Yan']",,
Knowledge-guided Open Attribute Value Extraction with Reinforcement Learning,"['Ye Liu', 'Sheng Zhang', 'Rui Song', 'Suo Feng', 'Yanghua Xiao']",,
Language Generation with Multi-hop Reasoning on Commonsense Knowledge Graph,"['Haozhe Ji', 'Pei Ke', 'Shaohan Huang', 'Furu Wei', 'Xiaoyan Zhu', 'Minlie Huang']",http://arxiv.org/abs/2009.11692v1,"Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation. Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph. We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation. In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph. We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge. We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation."
Language Model Prior for Low-Resource Neural Machine Translation,"['Christos Baziotis', 'Barry Haddow', 'Alexandra Birch']",http://arxiv.org/abs/2004.14928v1,"The scarcity of large parallel corpora is an important obstacle for neural machine translation. A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data. In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM). Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM ""disagrees"" with the LM. This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language. The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference. We present an analysis of the effects that different methods have on the distributions of the TM. Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data."
LAReQA: Language-agnostic answer retrieval from a multilingual pool,"['Uma Roy', 'Noah Constant', 'Rami Al-Rfou', 'Aditya Barua', 'Aaron Phillips', 'Yinfei Yang']",http://arxiv.org/abs/2004.05484v1,"We present LAReQA, a challenging new benchmark for language-agnostic answer retrieval from a multilingual candidate pool. Unlike previous cross-lingual tasks, LAReQA tests for ""strong"" cross-lingual alignment, requiring semantically related cross-language pairs to be closer in representation space than unrelated same-language pairs. Building on multilingual BERT (mBERT), we study different strategies for achieving strong alignment. We find that augmenting training data via machine translation is effective, and improves significantly over using mBERT out-of-the-box. Interestingly, the embedding baseline that performs the best on LAReQA falls short of competing baselines on zero-shot variants of our task that only target ""weak"" alignment. This finding underscores our claim that languageagnostic retrieval is a substantively new kind of cross-lingual evaluation."
Latent Geographical Factors for Analyzing the Evolution of Dialects in Contact,['Yugo Murawaki'],,
Learn to Cross-lingual Transfer with Meta Graph Learning Across Heterogeneous Languages,"['Zheng Li', 'Mukul Kumar', 'William Headden', 'Bing Yin', 'Ying Wei', 'Yu Zhang', 'Qiang Yang']",,
Learning a Cost-Effective Annotation Policy for Question Answering,"['Bernhard Kratzwald', 'Stefan Feuerriegel', 'Huan Sun']",http://arxiv.org/abs/2010.03476v1,"State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive. For this reason, customizing QA systems is challenging. As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme. The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations. Human annotators then simply provide binary feedback on these candidates. Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost. To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost. We compare our framework against traditional manual annotations in an extensive set of experiments. We find that our approach can reduce up to 21.1% of the annotation cost."
Learning a Simple and Effective Model for Multi-turn Response Generation with Auxiliary Tasks,"['YUFAN ZHAO', 'Can Xu', 'wei wu']",http://arxiv.org/abs/2004.01972v1,"We study multi-turn response generation for open-domain dialogues. The existing state-of-the-art addresses the problem with deep neural architectures. While these models improved response quality, their complexity also hinders the application of the models in real systems. In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation. To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation. By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum. Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process."
Learning Adaptive Segmentation Policy for Simultaneous Translation,"['Ruiqing Zhang', 'Chuanqiang Zhang', 'Zhongjun He', 'Hua Wu', 'Haifeng Wang']",,
Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification,"['Prithviraj Sen', 'Marina Danilevsky', 'Yunyao Li', 'Siddhartha Brahma', 'Matthias Boehm', 'Laura Chiticariu', 'Rajasekar Krishnamurthy']",,
Learning from Context or Names? An Empirical Study on Neural Relation Extraction,"['Hao Peng', 'Tianyu Gao', 'Xu Han', 'Yankai Lin', 'Peng Li', 'Zhiyuan Liu', 'Maosong Sun', 'Jie Zhou']",http://arxiv.org/abs/2010.01923v1,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names."
Learning from Task Descriptions,"['Orion Weller', 'Nicholas Lourie', 'Matt Gardner', 'Matthew Peters']",,
Learning Helpful Inductive Biases from Self-Supervised Pretraining,"['Alex Warstadt', 'Yian Zhang', 'Xiaocheng Li', 'Haokun Liu', 'Samuel R. Bowman']",,
Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models,"['Isabel Papadimitriou', 'Dan Jurafsky']",http://arxiv.org/abs/2004.14601v2,"We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. Training on artificial languages containing recursion (hierarchical structure) also improves performance on natural language, again with no vocabulary overlap. Surprisingly, training on artificial languages consisting of sets of separated pairs of words, but with no recursion, improves performance on natural language as well as recursive languages do. Experiments on transfer between natural languages show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced from natural languages correspond to the cross-linguistic syntactic properties studied in linguistic typology. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which a learner needs to model language."
Learning to Explain: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering,"['Harsh Jhamtani', 'Peter Clark']",http://arxiv.org/abs/2010.03274v1,"Despite the rapid progress in multihop question-answering (QA), models still have trouble explaining why an answer is correct, with limited explanation training data available to learn from. To address this, we introduce three explanation datasets in which explanations formed from corpus facts are annotated. Our first dataset, eQASC, contains over 98K explanation annotations for the multihop question answering dataset QASC, and is the first that annotates multiple candidate explanations for each answer. The second dataset eQASC-perturbed is constructed by crowd-sourcing perturbations (while preserving their validity) of a subset of explanations in QASC, to test consistency and generalization of explanation prediction models. The third dataset eOBQA is constructed by adding explanation annotations to the OBQA dataset to test generalization of models trained on eQASC. We show that this data can be used to significantly improve explanation quality (+14% absolute F1 over a strong retrieval baseline) using a BERT-based classifier, but still behind the upper bound, offering a new challenge for future research. We also explore a delexicalized chain representation in which repeated noun phrases are replaced by variables, thus turning them into generalized reasoning chains (for example: ""X is a Y"" AND ""Y has Z"" IMPLIES ""X has Z""). We find that generalized chains maintain performance while also being more robust to certain perturbations."
Learning to Pronounce Chinese Without a Pronunciation Dictionary,"['Christopher Chu', 'Scot Fang', 'Kevin Knight']",,
Learning to Represent Image and Text with Denotation Graphs,"['Bowen Zhang', 'Hexiang Hu', 'Vihan Jain', 'Eugene Ie', 'Fei Sha']",http://arxiv.org/abs/2010.02949v1,"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG."
Learning VAE-LDA Models with Rounded Reparameterization Trick,"['Runzhi Tian', 'Yongyi Mao', 'Richong Zhang']",,
Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers,"['Hanjie Chen', 'Yangfeng Ji']",http://arxiv.org/abs/2010.00667v1,"To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations. A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training. To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions. The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets. Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability."
Less is More: Attention Supervision with Counterfactuals for Text Classification,"['Seungtaek Choi', 'Haeju Park', 'Jinyoung Yeo', 'Seung-won Hwang']",,
Let's Stop Error Propagation in the End-to-End Relation Extraction Literature!,"['Bruno Taill√©', 'Vincent Guigue', 'Geoffrey Scoutheeten', 'patrick Gallinari']",,
Leveraging Declarative Knowledge in Text and First-Order Logic for Fine-Grained Propaganda Detection,"['Ruize Wang', 'Duyu Tang', 'Nan Duan', 'Wanjun Zhong', 'Zhongyu Wei', 'Xuanjing Huang', 'Daxin Jiang', 'Ming Zhou']",http://arxiv.org/abs/2004.14201v2,"We study the detection of propagandistic text fragments in news articles. Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques. Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language. The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions. The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters. We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection. Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions."
Lifelong Language Knowledge Distillation,"['Yung-Sung Chuang', 'Shang-Yu Su', 'Yun-Nung Chen']",http://arxiv.org/abs/2010.02123v1,"It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks."
"Lightweight, Dynamic Graph Convolutional Networks for AMR-to-Text Generation","['Yan Zhang', 'Zhijiang Guo', 'Zhiyang Teng', 'Wei Lu', 'Shay B. Cohen', 'ZUOZHU LIU', 'Lidong Bing']",,
Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions,"['Bodhisattwa Prasad Majumder', 'Harsh Jhamtani', 'Taylor Berg-Kirkpatrick', 'Julian McAuley']",http://arxiv.org/abs/2010.03205v1,"Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly. For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break. In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions. Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response. Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions. Our model outperforms competitive baselines on the PersonaChat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation."
LNMap: Departures from Isomorphic Assumption in Bilingual Lexicon Induction Through Non-Linear Mapping in Latent Space,"['Tasnim Mohiuddin', 'M Saiful Bari', 'Shafiq Joty']",http://arxiv.org/abs/2004.13889v1,"Most of the successful and predominant methods for bilingual lexicon induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages. In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI. Our model is independent of the isomorphic assumption and uses nonlinear mapping in the latent space of two independently trained auto-encoders. Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin. Ablation studies show the importance of different model components and the necessity of non-linear mapping."
Local Additivity Based Data Augmentation for Semi-supervised NER,"['Jiaao Chen', 'Zhenghui Wang', 'Ran Tian', 'Zichao Yang', 'Diyi Yang']",http://arxiv.org/abs/2010.01677v1,"Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data. In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other. Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate. Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning. We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data. Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines. We have publicly released our code at https://github.com/GT-SALT/LADA."
Localizing Q&A Semantic Parsers for Any Language In a Day,"['Mehrad Moradshahi', 'Giovanni Campagna', 'Sina Semnani', 'Silei Xu', 'Monica Lam']",,
Look at the First Sentence: Position Bias in Question Answering,"['Miyoung Ko', 'Jinhyuk Lee', 'Hyunjae Kim', 'Gangwoo Kim', 'Jaewoo Kang']",http://arxiv.org/abs/2004.14602v3,"Many extractive question answering models are trained to predict start and end positions of answers. The choice of predicting answers as positions is mainly due to its simplicity and effectiveness. In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions. We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT. To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling. Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset."
Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation,"['Maximiliana Behnke', 'Kenneth Heafield']",,
Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing,"['Xilun Chen', 'Asish Ghoshal', 'Yashar Mehdad', 'Luke Zettlemoyer', 'Sonal Gupta']",http://arxiv.org/abs/2010.03546v1,"Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user's intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public."
LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention,"['Ikuya Yamada', 'Akari Asai', 'Hiroyuki Shindo', 'Hideaki Takeda', 'Yuji Matsumoto']",http://arxiv.org/abs/2010.01057v1,"Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke."
MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer,"['Jonas Pfeiffer', 'Ivan Vuliƒá', 'Iryna Gurevych', 'Sebastian Ruder']",http://arxiv.org/abs/2005.00052v3,"The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml"
Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation,"['Nils Reimers', 'Iryna Gurevych']",http://arxiv.org/abs/2004.09813v2,"We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available."
Masking as an Efficient Alternative to Finetuning for Pretrained Language Models,"['Mengjie Zhao', 'Tao Lin', 'Fei Mi', 'Martin Jaggi', 'Hinrich Sch√ºtze']",http://arxiv.org/abs/2004.12406v1,"We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT and RoBERTa on a series of NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred simultaneously. Through intrinsic evaluations, we show that representations computed by masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning."
MAVEN: A Massive General Domain Event Detection Dataset,"['Xiaozhi Wang', 'Ziqi Wang', 'Xu Han', 'Wangyi Jiang', 'Rong Han', 'Zhiyuan Liu', 'Juanzi Li', 'Peng Li', 'Yankai Lin', 'Jie Zhou']",http://arxiv.org/abs/2004.13590v2,"Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text. Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity. Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods. (2) Low coverage. Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models. To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types. MAVEN alleviates the data scarcity problem and covers much more general event types. We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN. The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset."
Measuring Information Propagation in Literary Social Networks,"['Matthew Sims', 'David Bamman']",http://arxiv.org/abs/2004.13980v2,"We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text. We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied. Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men."
Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions,"['Arya D. McCarthy', 'Adina Williams', 'Shijia Liu', 'David Yarowsky', 'Ryan Cotterell']",,
MedDialog: A Large-scale Medical Dialogue Dataset,"['Guangtao Zeng', 'Wenmian Yang', 'Zeqian Ju', 'Yue Yang', 'Sicheng Wang', 'Ruisi Zhang', 'Meng Zhou', 'Jiaqi Zeng', 'Xiangyu Dong', 'Ruoyu Zhang', 'Hongchao Fang', 'Penghui Zhu', 'Shu Chen', 'Pengtao Xie']",,
MEGA RST Discourse Treebanks with Structure and Nuclearity from Scalable Distant Sentiment Supervision,"['Patrick Huber', 'Giuseppe Carenini']",,
Message Passing for Hyper-Relational Knowledge Graphs,"['Mikhail Galkin', 'Priyansh Trivedi', 'Gaurav Maheshwari', 'Ricardo Usbeck', 'Jens Lehmann']",http://arxiv.org/abs/2009.10847v1,"Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations."
Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,"['Chengyu Wang', 'Minghui Qiu', 'jun huang', 'XIAOFENG HE']",http://arxiv.org/abs/2003.13003v2,"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domain text mining tasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning."
META: Metadata-Empowered Weak Supervision for Text Classification,"['Dheeraj Mekala', 'Xinyang Zhang', 'Jingbo Shang']",,
Methods for Numeracy-Preserving Word Embeddings,"['Dhanasekar Sundararaman', 'Shijing Si', 'Vivek Subramanian', 'Guoyin Wang', 'Devamanyu Hazarika', 'Lawrence Carin']",,
MIME: MIMicking Emotions for Empathetic Response Generation,"['Navonil Majumder', 'Pengfei Hong', 'Shanshan Peng', 'Jiankun Lu', 'Deepanway Ghosal', 'Alexander Gelbukh', 'Rada Mihalcea', 'Soujanya Poria']",http://arxiv.org/abs/2010.01454v1,"Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly. We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content. We show that the consideration of this polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art. Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work. We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations. The implementation of MIME is publicly available at https://github.com/declare-lab/MIME."
Mind Your Inflections! Improving NLP for Non-Standard Englishes with Base-Inflection Encoding,"['Samson Tan', 'Shafiq Joty', 'Lav Varshney', 'Min-Yen Kan']",,
MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems,"['Zhaojiang Lin', 'Andrea Madotto', 'Genta Indra Winata', 'Pascale Fung']",http://arxiv.org/abs/2009.12005v2,"In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data. MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation. Unlike previous approaches, which use a copy mechanism to ""carryover"" the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length. We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ. Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20\% training data, and 3) Lev greatly improves the inference efficiency."
Mitigating Gender Bias for Neural Dialogue Generation with Adversarial Learning,"['Haochen Liu', 'Wentao Wang', 'Yiqi Wang', 'Hui Liu', 'Zitao Liu', 'Jiliang Tang']",http://arxiv.org/abs/2009.13028v1,"Dialogue systems play an increasingly important role in various aspects of our daily life. It is evident from recent research that dialogue systems trained on human conversation data are biased. In particular, they can produce responses that reflect people's gender prejudice. Many debiasing methods have been developed for various natural language processing tasks, such as word embedding. However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders. This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models. In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance. Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality."
MLSUM: The Multilingual Summarization Corpus,"['Thomas Scialom', 'Paul-Alexis Dray', 'Sylvain Lamprier', 'Benjamin Piwowarski', 'Jacopo Staiano']",http://arxiv.org/abs/2004.14900v1,"We present MLSUM, the first large-scale MultiLingual SUMmarization dataset. Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five different languages -- namely, French, German, Spanish, Russian, Turkish. Together with English newspapers from the popular CNN/Daily mail dataset, the collected data form a large scale multilingual dataset which can enable new research directions for the text summarization community. We report cross-lingual comparative analyses based on state-of-the-art systems. These highlight existing biases which motivate the use of a multi-lingual dataset."
MOCHA: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics,"['Anthony Chen', 'Gabriel Stanovsky', 'Sameer Singh', 'Matt Gardner']",http://arxiv.org/abs/2010.03636v1,"Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics."
MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification,"['Qianli Ma', 'Zhenxi Lin', 'Jiangyue Yan', 'Zipeng Chen', 'Liuhong Yu']",,
Modeling Protagonist Emotions for Emotion-Aware Storytelling,"['Faeze Brahman', 'Snigdha Chaturvedi']",,
Modeling the Music Genre Perception across Language-Bound Cultures,"['Elena V. Epure', 'Guillaume Salha', 'Manuel Moussallam', 'Romain Hennequin']",,
Modularized Transfomer-based Ranking Framework,"['Luyu Gao', 'Zhuyun Dai', 'Jamie Callan']",http://arxiv.org/abs/2004.13313v3,"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers."
"MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French","['AmirAli Bagher Zadeh', 'Yansheng Cao', 'Simon Hessner', 'Paul Pu Liang', 'Soujanya Poria', 'Louis-Philippe Morency']",,
MovieChats: Chat like Humans in a Closed Domain,"['Hui Su', 'Xiaoyu Shen', 'Zhou Xiao', 'Zheng Zhang', 'Ernie Chang', 'Cheng Zhang', 'Cheng Niu', 'Jie Zhou']",,
Multi-Dimensional Gender Bias Classification,"['Emily Dinan', 'Angela Fan', 'Ledell Wu', 'Jason Weston', 'Douwe Kiela', 'Adina Williams']",http://arxiv.org/abs/2005.00614v1,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness."
Multi-document Summarization with Maximal Marginal Relevance-guided Reinforcement Learning,"['Yuning Mao', 'Yanru Qu', 'Yiqing Xie', 'Xiang Ren', 'Jiawei Han']",http://arxiv.org/abs/2010.00117v1,"While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS). We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle. To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS. RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning. Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy. Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets. In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency."
Multi-Fact Correction in Abstractive Text Summarization,"['Yue Dong', 'Shuohang Wang', 'Zhe Gan', 'Yu Cheng', 'Jackie Chi Kit Cheung', 'Jingjing Liu']",http://arxiv.org/abs/2010.02443v1,"Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation."
Multi-hop Inference for Question-driven Summarization,"['Yang Deng', 'Wenxuan Zhang', 'Wai Lam']",http://arxiv.org/abs/2010.03738v1,"Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA."
Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis,"['Yuncong Li', 'Cunxiang Yin', 'Sheng-hua Zhong', 'Xu Pan']",http://arxiv.org/abs/2010.02656v1,"Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories. To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation. These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance. In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category. Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments. Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN."
Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,"['Dong Zhang', 'Xincheng Ju', 'Junhui Li', 'Shoushan Li', 'Qiaoming Zhu', 'Guodong Zhou']",,
Multi-resolution Annotations for Emoji Prediction,"['Weicheng Ma', 'Ruibo Liu', 'Lili Wang', 'Soroush Vosoughi']",,
Multi-Stage Pre-training for Automated Chinese Essay Scoring,"['Wei Song', 'Kai Zhang', 'Ruiji Fu', 'Lizhen Liu', 'Ting Liu', 'Miaomiao Cheng']",,
Multi-Step Inference for Reasoning Over Paragraphs,"['Jiangming Liu', 'Matt Gardner', 'Shay B. Cohen', 'Mirella Lapata']",http://arxiv.org/abs/2004.02995v1,"Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives. Prior work has largely tried to do this either symbolically or with black-box transformers. We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning. This model first finds relevant sentences in the context and then chains them together using neural modules. Our model gives significant performance improvements (up to 29\% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset"
Multi-Task Learning for Logically Dependent Tasks from the Perspective of Causal Inference,"['Wenqing Chen', 'Jidong Tian', 'Liqiang Xiao', 'Hao He', 'Yaohui Jin']",,
Multi-task Learning for Multilingual Neural Machine Translation,"['Yiren Wang', 'ChengXiang Zhai', 'Hany Hassan']",http://arxiv.org/abs/2010.02523v1,"While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task."
Multi-turn Response Selection using Dialogue Dependency Relations,"['Qi Jia', 'Yizhu Liu', 'Siyu Ren', 'Kenny Zhu', 'Haifeng Tang']",http://arxiv.org/abs/2010.01502v1,"Multi-turn response selection is a task designed for developing dialogue agents. The performance on this task has a remarkable improvement with pre-trained language models. However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns. In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations. Each thread can be regarded as a self-contained sub-dialogue. We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer. The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2."
Multi-Unit Transformers for Neural Machine Translation,"['Jianhao Yan', 'Fandong Meng', 'Jie Zhou']",,
Multi-View Sequence-to-Sequence Models with Conversational Structure for Abstractive Dialogue Summarization,"['Jiaao Chen', 'Diyi Yang']",http://arxiv.org/abs/2010.01672v1,"Text summarization is one of the most challenging and interesting problems in NLP. Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations---an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers---remains relatively under-investigated. This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries. Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment. We also discussed specific challenges that current approaches faced with this task. We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq."
Multi-view Story Characterization from Movie Plot Synopses and Reviews,"['Sudipta Kar', 'Gustavo Aguilar', 'Mirella Lapata', 'Thamar Solorio']",,
MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale,"['Andreas R√ºckl√©', 'Jonas Pfeiffer', 'Iryna Gurevych']",http://arxiv.org/abs/2010.00980v1,"We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks."
Multilevel Text Alignment with Cross-Document Attention,"['Xuhui Zhou', 'Nikolaos Pappas', 'Noah A. Smith']",http://arxiv.org/abs/2010.01263v1,"Text alignment finds application in tasks such as citation recommendation and plagiarism detection. Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels. We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document). Our component is weakly supervised from document pairs and can align at multiple levels. Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents."
Multilingual AMR-to-Text Generation,"['Angela Fan', 'Claire Gardent']",,
Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product,"['Tiangang Zhu', 'Yue Wang', 'Haoran Li', 'Youzheng Wu', 'Xiaodong He', 'Bowen Zhou']",http://arxiv.org/abs/2009.07162v1,"Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval. While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications. In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images. We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given. Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values. Moreover, product images have distinct effects on our tasks for different product attributes and values. Thus, we selectively draw useful visual information from product images to enhance our model. We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset will be released to the public."
Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis,"['Yao-Hung Hubert Tsai', 'Martin Ma', 'Muqiao Yang', 'Ruslan Salakhutdinov', 'Louis-Philippe Morency']",,
Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos,"['Nayu Liu', 'Xian Sun', 'Hongfeng Yu', 'Wenkai Zhang', 'Guangluan Xu']",,
MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering,"['Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral', 'Yezhou Yang']",http://arxiv.org/abs/2009.08566v1,"While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present \textit{MUTANT}, a training paradigm that exposes the model to perceptually similar, yet semantically distinct \textit{mutations} of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, \textit{MUTANT} does not rely on the knowledge about the nature of train and test answer distributions. \textit{MUTANT} establishes a new state-of-the-art accuracy on VQA-CP with a $10.57\%$ improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering."
Named Entity Recognition Only from Word Embeddings,"['Ying Luo', 'Hai Zhao', 'Junlang Zhan']",,
Natural Language Processing for Achieving Sustainable Development: the Case of Neural Labelling to Enhance Community Profiling,"['Costanza Conforti', 'Stephanie Hirmer', 'Dai Morgan', 'Marco Basaldella', 'Yau Ben Or']",http://arxiv.org/abs/2004.12935v1,"In recent years, there has been an increasing interest in the application of Artificial Intelligence - and especially Machine Learning - to the field of Sustainable Development (SD). However, until now, NLP has not been applied in this context. In this research paper, we show the high potential of NLP applications to enhance the sustainability of projects. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. In this context, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new task of Automatic UPV classification, which is an extreme multi-class multi-label classification problem. We release Stories2Insights, an expert-annotated dataset, provide a detailed corpus analysis, and implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leave plenty of room for future research at the intersection of NLP and SD."
Near-imperceptible Neural Linguistic Steganography via Self-Adjusting Arithmetic Coding,"['Jiaming Shen', 'Heng Ji', 'Jiawei Han']",http://arxiv.org/abs/2010.00677v1,"Linguistic steganography studies how to hide secret messages in natural language cover texts. Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification. Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message. In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model. We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively. Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers."
Neural Deepfake Detection with Factual Structure of Text,"['Wanjun Zhong', 'Duyu Tang', 'Zenan Xu', 'Ruize Wang', 'Nan Duan', 'Ming Zhou', 'Jiahai Wang', 'Jian Yin']",,
Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network,"['Ruipeng Jia', 'Yanan Cao', 'Hengzhu Tang', 'Fang Fang', 'Cong Cao', 'Shi Wang']",,
Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation,"['Minki Kang', 'Moonsu Han', 'Sung Ju Hwang']",http://arxiv.org/abs/2010.02705v1,"We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering). Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts. We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text. We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings."
Neural Topic Modeling with Cycle-Consistent Adversarial Training,"['Xuemeng Hu', 'Rui Wang', 'Deyu Zhou', 'Yuxuan Xiong']",http://arxiv.org/abs/2009.13971v1,"Advances on deep generative models have attracted significant research interest in neural topic modeling. The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics. It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels. To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT. ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics. Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other. sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics. The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification. The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines."
Non-Autoregressive Machine Translation with Latent Alignments,"['Chitwan Saharia', 'William Chan', 'Saurabh Saxena', 'Mohammad Norouzi']",http://arxiv.org/abs/2004.07437v2,"This paper investigates two latent alignment models for non-autoregressive machine translation, namely CTC and Imputer. CTC generates outputs in a single step, makes strong conditional independence assumptions about output variables, and marginalizes out latent alignments using dynamic programming. Imputer generates outputs in a constant number of steps, and approximately marginalizes out possible generation orders and latent alignments for training. These models are simpler than existing non-autoregressive methods, since they do not require output length prediction as a pre-process. In addition, our architecture is simpler than typical encoder-decoder architectures, since input-output cross attention is not used. On the competitive WMT'14 En$\rightarrow$De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps. This compares favourably to the baseline autoregressive Transformer with 27.8 BLEU."
"Not Low-Resource Anymore: Aligner Ensembling, Batch Filtering, and New Datasets for Bengali-English Machine Translation","['Tahmid Hasan', 'Abhik Bhattacharjee', 'Kazi Samin', 'Masum Hasan', 'Madhusudan Basak', 'M. Sohel Rahman', 'Rifat Shahriyar']",http://arxiv.org/abs/2009.09359v2,"Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources. Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them. In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering. With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before. Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation. We also evaluate on a new test set of 1000 pairs made with extensive quality control. We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status. To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation. We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages. Our data and code are available at https://github.com/csebuetnlp/banglanmt."
NwQM: A neural quality assessment framework for Wikipedia,"['Bhanu Prakash Reddy Guda', 'Sasi Bhushan Seelaboyina', 'Soumya Sarkar', 'Animesh Mukherjee']",,
OCR Post-Correction for Endangered Language Texts,"['Shruti Rijhwani', 'Antonios Anastasopoulos', 'Graham Neubig']",,
On Extractive and Abstractive Neural Document Summarization with Transformer Language Models,"['Jonathan Pilault', 'Raymond Li', 'Sandeep Subramanian', 'Chris Pal']",,
On Losses for Modern Language Models,"['St√©phane Aroca-Ouellette', 'Frank Rudzicz']",http://arxiv.org/abs/2010.01694v1,"BERT set many state-of-the-art results over varied NLU benchmarks by pre-training over two tasks: masked language modelling (MLM) and next sentence prediction (NSP), the latter of which has been highly criticized. In this paper, we 1) clarify NSP's effect on BERT pre-training, 2) explore fourteen possible auxiliary pre-training tasks, of which seven are novel to modern language models, and 3) investigate different ways to include multiple tasks into pre-training. We show that NSP is detrimental to training due to its context splitting and shallow semantic signal. We also identify six auxiliary pre-training tasks -- sentence ordering, adjacent sentence prediction, TF prediction, TF-IDF prediction, a FastSent variant, and a Quick Thoughts variant -- that outperform a pure MLM baseline. Finally, we demonstrate that using multiple tasks in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERT Base on the GLUE benchmark using fewer than a quarter of the training tokens."
On Negative Interference in Multilingual Language Models,"['Zirui Wang', 'Zachary C. Lipton', 'Yulia Tsvetkov']",,
On the Ability of Self-Attention Networks to Recognize Counter Languages,"['Satwik Bhattamishra', 'Kabir Ahuja', 'Navin Goyal']",,
On the Reliability and Validity of Detecting Approval of Political Actors in Tweets,"['Indira Sen', 'Fabian Fl√∂ck', 'Claudia Wagner']",,
On the Sentence Embeddings from BERT for Semantic Textual Similarity,"['Bohan Li', 'Hao Zhou', 'Junxian He', 'Mingxuan Wang', 'Yiming Yang', 'Lei Li']",,
Online Back-Parsing for AMR-to-Text Generation,"['Xuefeng Bai', 'Linfeng Song', 'Yue Zhang']",,
Online Conversation Disentanglement with Pointer Networks,"['Tao Yu', 'Shafiq Joty']",,
Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space,"['Chunyuan Li', 'Xiang Gao', 'Yuan Li', 'Baolin Peng', 'Xiujun Li', 'Yizhe Zhang', 'Jianfeng Gao']",http://arxiv.org/abs/2004.04092v3,"When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language. In this paper, we propose the first large-scale language VAE model, Optimus. A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks. Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors. Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure. Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus. It achieves new state-of-the-art on VAE language modeling benchmarks. We hope that our first pre-trained big VAE language model itself and results can help the NLP community renew the interests of deep generative models in the era of large-scale pre-training, and make these principled methods more practical."
PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation,"['Xinyu Hua', 'Lu Wang']",http://arxiv.org/abs/2010.02301v1,"Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often ""rambling"" without coherently arranged content. In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART. We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions. The BART model is employed for generation without modifying its structure. We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning."
PALM: Pre-training an Autoencoding&autoregressive Language Model for Context-conditioned Generation,"['Bin Bi', 'Chenliang Li', 'Chen Wu', 'Ming Yan', 'Wei Wang', 'Songfang Huang', 'Fei Huang', 'Luo Si']",http://arxiv.org/abs/2004.07159v2,"Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation. Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens. The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context. This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context. The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues."
PARADE: A New Dataset for Paraphrase Identification Requiring Computer Science Domain Knowledge,"['Yun He', 'Zhuoer Wang', 'Yin Zhang', 'Ruihong Huang', 'James Caverlee']",http://arxiv.org/abs/2010.03725v1,"We present a new benchmark dataset called PARADE for paraphrase identification that requires specialized domain knowledge. PARADE contains paraphrases that overlap very little at the lexical and syntactic level but are semantically equivalent based on computer science domain knowledge, as well as non-paraphrases that overlap greatly at the lexical and syntactic level but are not semantically equivalent based on this domain knowledge. Experiments show that both state-of-the-art neural models and non-expert human annotators have poor performance on PARADE. For example, BERT after fine-tuning achieves an F1 score of 0.709, which is much lower than its performance on other paraphrase identification datasets. PARADE can serve as a resource for researchers interested in testing models that incorporate domain knowledge. We make our data and code freely available."
Parallel Interactive Networks for Multi-Domain Dialogue State Generation,"['Junfan Chen', 'Richong Zhang', 'Yongyi Mao', 'Jie Xu']",http://arxiv.org/abs/2009.07616v2,"The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models. In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies. Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies. The slot-level context is introduced to extract more expressive features for different slots. And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances. Empirical studies demonstrated the superiority of the proposed PIN model."
Pareto Probing: Trading-Off Accuracy and Complexity,"['Tiago Pimentel', 'Naomi Saphra', 'Adina Williams', 'Ryan Cotterell']",http://arxiv.org/abs/2010.02180v1,"The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention. In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments with such metrics show that probe's performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones). These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations. We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume. In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations."
Partially-Aligned Data-to-Text Generation with Distant Supervision,"['Zihao Fu', 'Bei Shi', 'Wai Lam', 'Lidong Bing', 'Zhiyuan Liu']",http://arxiv.org/abs/2010.01268v1,"The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability. However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain. Using partially-aligned data is an alternative way of solving the dataset scarcity problem. This kind of data is much easier to obtain since it can be produced automatically. However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure. In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains. To tackle this new task, we propose a novel distant supervision generation framework. It firstly estimates the input data's supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively. We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata. The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data."
PathQG: Neural Question Generation from Facts,"['Siyuan Wang', 'Zhongyu Wei', 'Zhihao Fan', 'Zengfeng Huang', 'Weijian Sun', 'Qi ZHANG', 'Xuanjing Huang']",,
Personal Information Leakage Detection in Conversations,"['Qiongkai Xu', 'Lizhen Qu', 'Zeyu Gao', 'Gholamreza Haffari']",,
Planning and Generating Natural and Diverse Disfluent Texts as Augmentation for Disfluency Detection,"['Jingfeng Yang', 'Diyi Yang', 'Zhaoran Ma']",,
PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,"['Hannah Rashkin', 'Asli Celikyilmaz', 'Yejin Choi', 'Jianfeng Gao']",http://arxiv.org/abs/2004.14967v1,"We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline. This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by weaving through the key points provided in the outline. This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story. We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states. In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different styles of writing corresponding to different parts of the narrative. Comprehensive experiments over three fiction and non-fiction datasets demonstrate that recently introduced large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots."
Plug and Play Autoencoders for Conditional Text Generation,"['Florian Mai', 'Nikolaos Pappas', 'Ivan Montero', 'Noah A. Smith', 'James Henderson']",http://arxiv.org/abs/2010.02983v1,"Text autoencoders are commonly used for conditional generation tasks such as style transfer. We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder's embedding space, training embedding-to-embedding (Emb2Emb). This reduces the need for labeled training data for the task and makes the training procedure more efficient. Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors. Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster."
Point to the Expression: Solving Algebraic Word Problems using the Expression-Pointer Transformer Model,"['Bugeun Kim', 'Kyung Seo Ki', 'Donggeon Lee', 'Gahgene Gweon']",,
Pointer: Constrained Text Generation via Insertion-based Generative Pre-training,"['Yizhe Zhang', 'Guoyin Wang', 'Chunyuan Li', 'Zhe Gan', 'Chris Brockett', 'Bill Dolan']",http://arxiv.org/abs/2005.00558v2,"Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields an empirically logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that POINTER achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research (https://github.com/dreasysnail/POINTER)."
Position-Aware Tagging for Aspect Sentiment Triplet Extraction,"['Lu Xu', 'Hao Li', 'Wei Lu', 'Lidong Bing']",http://arxiv.org/abs/2010.02609v1,"Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment. Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages. Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach. However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question. In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets. Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches. We also conducted extensive experiments to investigate the model effectiveness and robustness."
PowerTransformer: Unsupervised Controllable Revision for Biased Language Correction,"['Xinyao Ma', 'Maarten Sap', 'Hannah Rashkin', 'Yejin Choi']",,
Pre-tokenization of Multi-word Expressions in Cross-lingual Word Embeddings,"['Naoki Otani', 'Satoru Ozaki', 'Xingyuan Zhao', 'Yucen Li', 'Micaelah St Johns', 'Lori Levin']",,
Pre-training Entity Relation Encoder with Intra-span and Inter-span Information,"['Yijun Wang', 'Changzhi Sun', 'Yuanbin Wu', 'Junchi Yan', 'Peng Gao', 'Guotong Xie']",,
Pre-training for Abstractive Document Summarization by Reinstating Source Text,"['Yanyan Zou', 'Xingxing Zhang', 'Wei Lu', 'Furu Wei', 'Ming Zhou']",http://arxiv.org/abs/2004.01853v3,"Abstractive document summarization is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Unfortunately, training large Seq2Seq based summarization models on limited supervised summarization data is challenging. This paper presents three pre-training objectives which allow us to pre-train a Seq2Seq based abstractive summarization model on unlabeled text. The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document. These objectives include sentence reordering, next sentence generation, and masked document generation, which have close relations with the abstractive document summarization task. Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines. Compared to models pre-trained on large-scale data (more than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness."
Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information,"['Zehui Lin', 'Xiao Pan', 'Mingxuan Wang', 'Xipeng Qiu', 'Jiangtao Feng', 'Hao Zhou', 'Lei Li']",http://arxiv.org/abs/2010.03142v1,"We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs? We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model. Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space. We pre-train a mRASP model on 32 language pairs jointly with only public datasets. The model is then fine-tuned on downstream language pairs to obtain specialized MT models. We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs. Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs. It is the first time to verify that multiple low-resource language pairs can be utilized to improve rich resource MT. Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pre-training corpus. Code, data, and pre-trained models are available at https://github.com/linzehui/mRASP."
Predicting Clinical Trial Results by Implicit Evidence Integration,"['Qiao Jin', 'Chuanqi Tan', 'Mosha Chen', 'Xiaozhong Liu', 'Songfang Huang']",,
Predicting Stance and Rumor Veracity via Dual Hierarchical Transformer with Pretrained Encoders,"['Jianfei Yu', 'Jing Jiang', 'Ling Min Serena Khoo', 'Hai Leong Chieu', 'Rui Xia']",,
Probing Pretrained Language Models for Lexical Semantics,"['Ivan Vuliƒá', 'Edoardo Maria Ponti', 'Robert Litschko', 'Goran Glava≈°', 'Anna Korhonen']",,
Probing Task-Oriented Dialogue Representation from Language Models,"['Chien-Sheng Wu', 'Caiming Xiong']",,
Profile Consistency Identification for Open-domain Dialogue Agents,"['Haoyu Song', 'Yan Wang', 'Wei-Nan Zhang', 'Zhengyu Zhao', 'Ting Liu', 'Xiaojiang Liu']",http://arxiv.org/abs/2009.09680v2,"Maintaining a consistent attribute profile is crucial for dialogue agents to naturally converse with humans. Existing studies on improving attribute consistency mainly explored how to incorporate attribute information in the responses, but few efforts have been made to identify the consistency relations between response and attribute profile. To facilitate the study of profile consistency identification, we create a large-scale human-annotated dataset with over 110K single-turn conversations and their key-value attribute profiles. Explicit relation between response and profile is manually labeled. We also propose a key-value structure information enriched BERT model to identify the profile consistency, and it gained improvements over strong baselines. Further evaluations on downstream tasks demonstrate that the profile consistency identification model is conducive for improving dialogue consistency."
Program Enhanced Fact Verification with Verbalization and Graph Attention Network,"['Xiaoyu Yang', 'Feng Nie', 'Yufei Feng', 'Quan Liu', 'Zhigang Chen', 'Xiaodan Zhu']",http://arxiv.org/abs/2010.03084v1,"Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding. In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models. Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables. Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision. To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results. Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT."
ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,"['Michael Boratko', 'Xiang Li', ""Tim O'Gorman"", 'Rajarshi Das', 'Dan Le', 'Andrew McCallum']",http://arxiv.org/abs/2005.00771v2,"Given questions regarding some prototypical situation such as Name something that people usually do before they leave the house for work? a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show FAMILY- FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task."
PRover: Proof Generation for Interpretable Reasoning over Rules,"['Swarnadeep Saha', 'Sayan Ghosh', 'Shashank Srivastava', 'Mohit Bansal']",http://arxiv.org/abs/2010.02830v1,"Recent work by Clark et al. (2020) shows that transformers can act as 'soft theorem provers' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PROVER generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PROVER obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for 'depth 5', indicating significant scope for future work. Our code and models are publicly available at https://github.com/swarnaHub/PRover"
PyMT5: multi-mode translation of natural language and Python code with transformers,"['Colin Clement', 'Dawn Drain', 'Jonathan Timcheck', 'Alexey Svyatkovskiy', 'Neel Sundaresan']",,
Q-learning with Language Model for Edit-based Unsupervised Summarization,"['Ryosuke Kohita', 'Akifumi Wachi', 'Yang Zhao', 'Ryuki Tachibana']",,
"QADiscourse - Discourse Relations as QA Pairs: Representation, Crowdsourcing and Baselines","['Valentina Pyatkin', 'Ayal Klein', 'Reut Tsarfaty', 'Ido Dagan']",,
Quantifying Intimacy In Language,"['Jiaxin Pei', 'David Jurgens']",,
Quantitative Argument Summarization and Beyond: Cross-Domain Key Point Analysis,"['Roy Bar-Haim', 'Yoav Kantor', 'Lilach Eden', 'Roni Friedman', 'Dan Lahav', 'Noam Slonim']",,
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"['Emily Dinan', 'Angela Fan', 'Adina Williams', 'Jack Urbanek', 'Douwe Kiela', 'Jason Weston']",http://arxiv.org/abs/1911.03842v2,"Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses."
Question Directed Graph Attention Network for Numerical Reasoning over Text,"['Kunlong Chen', 'Weidi Xu', 'Xingyi Cheng', 'Zou Xiaochuan', 'Yuyu Zhang', 'Le Song', 'Taifeng Wang', 'Yuan Qi', 'Wei Chu']",http://arxiv.org/abs/2009.07448v1,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph."
Re-evaluating Evaluation in Text Summarization,"['Manik Bhandari', 'Pranav Narayan Gour', 'Atabak Ashfaq', 'Pengfei Liu', 'Graham Neubig']",,
Re-examining the Role of Schema Linking in Text-to-SQL,"['Wenqiang Lei', 'Weixin Wang', 'Zhixin MA', 'Tian Gan', 'Wei Lu', 'Min-Yen Kan', 'Tat-Seng Chua']",,
Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,"['Sanyuan Chen', 'Yutai Hou', 'Yiming Cui', 'Wanxiang Che', 'Ting Liu', 'Xiangzhan Yu']",http://arxiv.org/abs/2004.12651v1,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community."
Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs,"['Woojeong Jin', 'Meng Qu', 'Xisen Jin', 'Xiang Ren']",http://arxiv.org/abs/1904.05530v4,"Knowledge graph reasoning is a critical task in natural language processing. The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp. Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future. This paper proposes Recurrent Event Network (RE-NET), a novel autoregressive architecture for predicting future interactions. The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs. Specifically, our RE-NET employs a recurrent event encoder to encode past facts and uses a neighborhood aggregator to model the connection of facts at the same timestamp. Future facts can then be inferred in a sequential manner based on the two modules. We evaluate our proposed method via link prediction at future times on five public datasets. Through extensive experiments, we demonstrate the strength of RENET, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets. Code and data can be found at https://github.com/INK-USC/RE-Net."
Recurrent Interaction Network for Jointly Extracting Entities and Classifying Relations,"['Kai Sun', 'Richong Zhang', 'Samuel Mensah', 'Yongyi Mao', 'xudong Liu']",http://arxiv.org/abs/2005.00162v2,"The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task. Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction. However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks. As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification. Empirical studies on two real-world datasets confirm the superiority of the proposed model."
"Refer, Reuse, Reduce: Grounding Subsequent References in Visual and Conversational Contexts","['Ece Takmaz', 'Mario Giulianelli', 'Sandro Pezzelle', 'Arabella Sinclair', 'Raquel Fern√°ndez']",,
Reformulating Unsupervised Style Transfer as Paraphrase Generation,"['Kalpesh Krishna', 'John Wieting', 'Mohit Iyyer']",,
Regularizing Dialogue Generation by Imitating Implicit Scenarios,"['Shaoxiong Feng', 'Xuancheng Ren', 'Hongshen Chen', 'Bin Sun', 'Kan Li', 'Xu SUN']",http://arxiv.org/abs/2010.01893v2,"Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario. To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge. More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference. Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge."
Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations,"['Taichi Ishiwatari', 'Yuki Yasuda', 'Taro Miyazaki', 'Jun Goto']",,
Repulsive Attention: Rethinking Multi-head Attention as Bayesian Inference,"['Bang An', 'Jie Lyu', 'Zhenyi Wang', 'Chunyuan Li', 'Changwei Hu', 'Fei Tan', 'Ruiyi Zhang', 'Yifan Hu', 'Changyou Chen']",http://arxiv.org/abs/2009.09364v1,"The neural attention mechanism plays an important role in many natural language processing applications. In particular, the use of multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives. Without explicit constraining, however, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model's representation power. In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective. Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model's expressiveness. Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention. Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on various tasks."
Response Selection for Multi-Party Conversations with Dynamic Topic Tracking,"['Weishi Wang', 'Steven C.H. Hoi', 'Shafiq Joty']",,
Retrofitting Structure-aware Transformer Language Model for End Tasks,"['Hao Fei', 'Yafeng Ren', 'Donghong Ji']",http://arxiv.org/abs/2009.07408v1,"We consider retrofitting structure-aware Transformer-based language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks."
Revisiting modularized multilingual NMT to meet industrial demands,"['Sungwon Lyu', 'Bokyung Son', 'Kichang Yang', 'Jaekyoung Bae']",,
RiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz Dataset with Rich Semantic Annotations for Task-Oriented Dialogue Modeling,"['Jun Quan', 'Shian Zhang', 'Qian Cao', 'Zizhong Li', 'Deyi Xiong']",,
RNNs can generate bounded hierarchical languages with optimal memory,"['John Hewitt', 'Michael Hahn', 'Surya Ganguli', 'Percy Liang', 'Christopher D. Manning']",,
Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,"['Alexander Ku', 'Peter Anderson', 'Roma Patel', 'Eugene Ie', 'Jason Baldridge']",,
Routing Enforced Generative Model for Recipe Generation,"['Zhiwei Yu', 'Hongyu Zang', 'Xiaojun Wan']",,
RussianSuperGLUE: A Russian Language Understanding Evaluation Benchmark,"['Tatiana Shavrina', 'Alena Fenogenova', 'Emelyanov Anton', 'Denis Shevelev', 'Ekaterina Artemova', 'Valentin Malykh', 'Vladislav Mikhailov', 'Maria Tikhonova', 'Andrey Chertok', 'Andrey Evlampiev']",,
Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering,"['Yanlin Feng', 'Xinyue Chen', 'Bill Yuchen Lin', 'Peifeng Wang', 'Jun Yan', 'Xiang Ren']",http://arxiv.org/abs/2005.00646v2,"Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies."
Scalable Zero-shot Entity Linking with Dense Entity Retrieval,"['Ledell Wu', 'Fabio Petroni', 'Martin Josifoski', 'Sebastian Riedel', 'Luke Zettlemoyer']",http://arxiv.org/abs/1911.03814v3,"This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off. We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description. The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions. Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text. Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables). We also show that bi-encoder linking is very fast with nearest neighbour search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation. Our code and models are available at https://github.com/facebookresearch/BLINK."
Scene Restoring for Narrative Machine Reading Comprehension,"['Zhixing Tian', 'Yuanzhe Zhang', 'Kang Liu', 'Jun Zhao', 'Yantao Jia', 'Zhicheng Sheng']",,
Selection and Generation: Learning towards Multi-Product Advertisement Post Generation,"['Zhangming Chan', 'Yuchi Zhang', 'Xiuying Chen', 'Shen Gao', 'Zhiqiang Zhang', 'Dongyan Zhao', 'Rui Yan']",,
Self-Induced Curriculum Learning in Self-Supervised Neural Machine Translation,"['Dana Ruiter', 'Josef van Genabith', 'Cristina Espa√±a-Bonet']",http://arxiv.org/abs/2004.03151v2,"Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle. In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training. We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum. We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance. We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students."
Self-Supervised Knowledge Triplet Learning for Zero-shot Question Answering,"['Pratyay Banerjee', 'Chitta Baral']",http://arxiv.org/abs/2005.00316v2,"The aim of all Question Answering (QA) systems is to be able to generalize to unseen questions. Current supervised methods are reliant on expensive data annotation. Moreover, such annotations can introduce unintended annotator bias which makes systems focus more on the bias than the actual task. In this work, we propose Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs. We propose heuristics to create synthetic graphs for commonsense and scientific knowledge. We propose methods of how to use KTL to perform zero-shot QA and our experiments show considerable improvements over large pre-trained transformer models."
Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks,"['Trapit Bansal', 'Rishikesh Jha', 'Tsendsuren Munkhdalai', 'Andrew McCallum']",,
Self-Supervised Text Planning for Paragraph Completion Task,"['Dongyeop Kang', 'Eduard Hovy']",,
SelfORE: Self-supervised Relational Feature Learning for Open Relation Extraction,"['Xuming Hu', 'Lijie Wen', 'Yusong Xu', 'Chenwei Zhang', 'Philip Yu']",http://arxiv.org/abs/2004.02438v2,"Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power. In this work, we proposed a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification. Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines."
Semantic Evaluation for Text-to-SQL with Distilled Test Suite,"['Ruiqi Zhong', 'Tao Yu', 'Dan Klein']",http://arxiv.org/abs/2010.02840v1,"We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available."
Semantic Role Labeling as Syntactic Dependency Parsing,"['Tianze Shi', 'Igor Malioutov', 'Ozan Irsoy']",,
Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems,"['Jinghui Qin', 'Lihui Lin', 'Xiaodan Liang', 'Rumin Zhang', 'Liang Lin']",,
Semi-Supervised Bilingual Lexicon Induction with Two-way Interaction,"['Xu Zhao', 'Zihao Wang', 'Hao Wu', 'Yong Zhang']",,
SentiLARE: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis,"['Pei Ke', 'Haozhe Ji', 'Siyang Liu', 'Xiaoyan Zhu', 'Minlie Huang']",,
Sentiment Analysis of Tweets using Heterogeneous Multi-layer Network Representation and Embedding,"['Loitongbam Gyanendro Singh', 'Anasua Mitra', 'Sanasam Ranbir Singh']",,
Seq2Edits: Sequence Transduction Using Span-level Edit Operations,"['Felix Stahlberg', 'Shankar Kumar']",http://arxiv.org/abs/2009.11136v1,"We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts. In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged. We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board. For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens. For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag."
SeqMix: Augmenting Active Sequence Labeling via Sequence Mixup,"['Rongzhi Zhang', 'Yue Yu', 'Chao Zhang']",http://arxiv.org/abs/2010.02322v1,"Active learning is an important technique for low-resource sequence labeling tasks. However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations. We propose a simple but effective data augmentation method to improve the label efficiency of active sequence labeling. Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration. The key difficulty is to generate plausible sequences along with token-level labels. In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples. Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not. Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by $2.27\%$--$3.75\%$ in terms of $F_1$ scores. The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix"
Sequential Modelling of the Evolution of Word Representations for Semantic Change Detection,"['Adam Tsakalidis', 'Maria Liakata']",,
SetConv: A New Approach for Learning from Imbalanced Data,"['Yang Gao', 'Yi-Fan Li', 'Yu Lin', 'Charu Aggarwal', 'Latifur Khan']",,
Shallow-to-Deep Training for Neural Machine Translation,"['Bei Li', 'Ziyang Wang', 'Hui Liu', 'Yufan Jiang', 'Quan Du', 'Tong Xiao', 'Huizhen Wang', 'Jingbo Zhu']",http://arxiv.org/abs/2010.03737v1,"Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT'16 English-German and WMT'14 English-French translation tasks show that it is $1.4$ $\times$ faster than training from scratch, and achieves a BLEU score of $30.33$ and $43.29$ on two tasks. The code is publicly available at https://github.com/libeineu/SDT-Training/."
Short Text Topic Modeling with Topic Distribution Quantization and Negative Sampling Decoder,"['Xiaobao Wu', 'Chunping Li', 'Yan Zhu', 'Yishu Miao']",,
Simultaneous Machine Translation with Visual Context,"['Ozan Caglayan', 'Julia Ive', 'Veneta Haralampieva', 'Pranava Madhyastha', 'Lo√Øc Barrault', 'Lucia Specia']",http://arxiv.org/abs/2009.07310v2,"Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible. The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation. In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context. To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks. Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios. Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French."
SLM: Learning a Discourse Language Representation with Sentence Unshuffling,"['Haejun Lee', 'Drew A. Hudson', 'Kangwook Lee', 'Christopher D. Manning']",,
Slot Attention with Value Normalization for Multi-domain Dialogue State Tracking,"['Yexiang Wang', 'Yi Guo', 'Siqi Zhu']",,
SLURP: A Spoken Language Understanding Resource Package,"['Emanuele Bastianelli', 'Andrea Vanzo', 'Pawel Swietojanski', 'Verena Rieser']",,
Social Chemistry 101: Learning to Reason about Social and Moral Norms,"['Maxwell Forbes', 'Jena D. Hwang', 'Vered Shwartz', 'Maarten Sap', 'Yejin Choi']",,
Social Media Attributions in the Context of Water Crisis,"['Rupak Sarkar', 'Sayantan Mahinder', 'Hirak Sarkar', 'Ashiqur KhudaBukhsh']",http://arxiv.org/abs/2001.01697v1,"Attribution of natural disasters/collective misfortune is a widely-studied political science problem. However, such studies are typically survey-centric or rely on a handful of experts to weigh in on the matter. In this paper, we explore how can we use social media data and an AI-driven approach to complement traditional surveys and automatically extract attribution factors. We focus on the most-recent Chennai water crisis which started off as a regional issue but rapidly escalated into a discussion topic with global importance following alarming water-crisis statistics. Specifically, we present a novel prediction task of attribution tie detection which identifies the factors held responsible for the crisis (e.g., poor city planning, exploding population etc.). On a challenging data set constructed from YouTube comments (72,098 comments posted by 43,859 users on 623 relevant videos to the crisis), we present a neural classifier to extract attribution ties that achieved a reasonable performance (Accuracy: 81.34\% on attribution detection and 71.19\% on attribution resolution)."
Solving Historical Dictionary Codes with a Neural Language Model,"['Christopher Chu', 'Raphael Valenti', 'Kevin Knight']",,
Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n^6) down to O(n^3),['Caio Corro'],http://arxiv.org/abs/2003.13785v1,"We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from $\mathcal O(n^6)$ down to $\mathcal O(n^3)$. The cubic time variant covers 98\% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger and Discontinuous PTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and \bert{}-based neural networks."
Sparse Parallel Training for Hierarchical Dirichlet Process Topic Models,"['Alexander Terenin', 'M√•ns Magnusson', 'Leif Jonsson']",http://arxiv.org/abs/1906.02416v2,"To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems. In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model. Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model. This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient. We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days."
Sparse Text Generation,"['Pedro Henrique Martins', 'Zita Marinho', 'Andr√© F. T. Martins']",http://arxiv.org/abs/2004.02644v3,"Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance. However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-$k$ or nucleus sampling. This creates a mismatch between training and testing conditions. In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch. The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text. In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: $\epsilon$-perplexity, sparsemax score, and Jensen-Shannon divergence. Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations."
Sparsity Makes Sense: Word Sense Disambiguation Using Sparse Contextualized Word Representations,['G√°bor Berend'],,
Speakers Fill Semantic Gaps with Context,"['Tiago Pimentel', 'Rowan Hall Maudslay', 'Damian Blasi', 'Ryan Cotterell']",http://arxiv.org/abs/2010.02172v1,"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear---resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this---one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. $\rho = 0.40$ in English). We then test our main hypothesis---that a word's lexical ambiguity should negatively correlate with its contextual uncertainty---and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative."
Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems,"['Jan Deriu', 'Don Tuggener', 'Pius von D√§niken', 'Jon Ander Campos', 'Alvaro Rodrigo', 'Thiziri Belkacem', 'Aitor Soroa', 'Eneko Agirre', 'Mark Cieliebak']",http://arxiv.org/abs/2010.02140v1,"The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce \emph{Spot The Bot}, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chatbots regarding their ability to mimic the conversational behavior of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chatbot can uphold human-like behavior the longest, i.e., \emph{Survival Analysis}. This metric has the ability to correlate a bot's performance to certain of its characteristics (e.g., \ fluency or sensibleness), yielding interpretable results. The comparably low cost of our framework allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying \emph{Spot The Bot} to three domains, evaluating several state-of-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool."
SRLGRN: Semantic Role Labeling Graph Reasoning Network,"['Chen Zheng', 'Parisa Kordjamshidi']",http://arxiv.org/abs/2010.03604v1,"This work deals with the challenge of learning and reasoning over multi-hop question answering (QA). We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly. The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges. Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths. Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models."
SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness,"['Nathan Ng', 'Kyunghyun Cho', 'Marzyeh Ghassemi']",http://arxiv.org/abs/2009.10195v2,"Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% accuracy on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English."
Stepwise Extractive Summarization and Planning with Structured Transformers,"['Shashi Narayan', 'Joshua Maynez', 'Jakub Adamek', 'Daniele Pighin', 'Blaz Bratanic', 'Ryan McDonald']",http://arxiv.org/abs/2010.02744v1,"We propose encoder-centric stepwise models for extractive summarization using structured transformers -- HiBERT and Extended Transformers. We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure. Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks. When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering. This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges."
STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering,"['Hrituraj Singh', 'Sumit Shekhar']",,
Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models,"['Ethan Wilcox', 'Peng Qian', 'Richard Futrell', 'Ryosuke Kohita', 'Roger Levy', 'Miguel Ballesteros']",,
Structured Attention for Unsupervised Dialogue Structure Induction,"['Liang Qiu', 'Yizhou Zhao', 'Weiyan Shi', 'Yuan Liang', 'Feng Shi', 'Tao Yuan', 'Zhou Yu', 'Song-Chun Zhu']",http://arxiv.org/abs/2009.08552v1,"Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics. Advancement made in this area is critical for dialogue system design and discourse analysis. It can also be extended to solve grammatical inference. In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion. Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias. Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus. While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation."
Structured Pruning of Large Language Models,"['Ziheng Wang', 'Jeremy Wohlwend', 'Tao Lei']",http://arxiv.org/abs/1910.04732v1,"Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks."
Sub-Instruction Aware Vision-and-Language Navigation,"['Yicong Hong', 'Cristian Rodriguez', 'Qi Wu', 'Stephen Gould']",http://arxiv.org/abs/2004.02707v2,"Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R."
SubjQA: A Dataset for Subjectivity and Review Comprehension,"['Johannes Bjerva', 'Nikita Bhutani', 'Behzad Golshan', 'Wang-Chiew Tan', 'Isabelle Augenstein']",http://arxiv.org/abs/2004.14283v3,"Subjectivity is the expression of internal opinions or beliefs which cannot be objectively observed or verified, and has been shown to be important for sentiment analysis and word-sense disambiguation. Furthermore, subjectivity is an important aspect of user-generated data. In spite of this, subjectivity has not been investigated in contexts where such data is widespread, such as in question answering (QA). We therefore investigate the relationship between subjectivity and QA, while developing a new dataset. We compare and contrast with analyses from previous work, and verify that findings regarding subjectivity still hold when using recently developed NLP architectures. We find that subjectivity is also an important feature in the case of QA, albeit with more intricate interactions between subjectivity and QA performance. For instance, a subjective question may or may not be associated with a subjective answer. We release an English QA dataset (SubjQA) based on customer reviews, containing subjectivity annotations for questions and answer spans across 6 distinct domains."
Substance over Style: Document-Level Targeted Content Transfer,"['Allison Hegel', 'Sudha Rao', 'Asli Celikyilmaz', 'Bill Dolan']",,
Surprisal Predicts Code-Switching in Chinese-English Bilingual Text,"['Jes√∫s Calvillo', 'Le Fang', 'Jeremy Cole', 'David Reitter']",,
SynSetExpan: An Iterative Framework for Joint Entity Set Expansion and Synonym Discovery,"['Jiaming Shen', 'Wenda Qiu', 'Jingbo Shang', 'Michelle Vanni', 'Xiang Ren', 'Jiawei Han']",http://arxiv.org/abs/2009.13827v1,"Entity set expansion and synonym discovery are two critical NLP tasks. Previous studies accomplish them separately, without exploring their interdependencies. In this work, we hypothesize that these two tasks are tightly coupled because two synonymous entities tend to have similar likelihoods of belonging to various semantic classes. This motivates us to design SynSetExpan, a novel framework that enables two tasks to mutually enhance each other. SynSetExpan uses a synonym discovery model to include popular entities' infrequent synonyms into the set, which boosts the set expansion recall. Meanwhile, the set expansion model, being able to determine whether an entity belongs to a semantic class, can generate pseudo training data to fine-tune the synonym discovery model towards better accuracy. To facilitate the research on studying the interplays of these two tasks, we create the first large-scale Synonym-Enhanced Set Expansion (SE2) dataset via crowdsourcing. Extensive experiments on the SE2 dataset and previous benchmarks demonstrate the effectiveness of SynSetExpan for both entity set expansion and synonym discovery tasks."
Systematic Comparison of Neural Architectures and Training Approaches for Open Information Extraction,"['Patrick Hohenecker', 'Frank Mtumbuka', 'Vid Kocijan', 'Thomas Lukasiewicz']",,
T3: Tree-Autoencoder Regularized Adversarial Text Generation for Targeted Attack,"['Boxin Wang', 'Hengzhi Pei', 'Boyuan Pan', 'Qian Chen', 'Shuohang Wang', 'Bo Li']",http://arxiv.org/abs/1912.10375v2,"Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models. Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models. Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable. To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks. In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation. A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP tasks: sentiment analysis and question answering (QA). Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human. Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice. Our work sheds light on an effective and general way to examine the robustness of NLP models. Our code is publicly available at https://github.com/AI-secure/T3/."
Tackling the Low-resource Challenge for Canonical Segmentation,"['Manuel Mager', '√ñzlem √áetinoƒülu', 'Katharina Kann']",http://arxiv.org/abs/2010.02804v1,"Canonical morphological segmentation consists of dividing words into their standardized morphemes. Here, we are interested in approaches for the task when training data is limited. We compare model performance in a simulated low-resource setting for the high-resource languages German, English, and Indonesian to experiments on new datasets for the truly low-resource languages Popoluca and Tepehua. We explore two new models for the task, borrowing from the closely related area of morphological generation: an LSTM pointer-generator and a sequence-to-sequence model with hard monotonic attention trained with imitation learning. We find that, in the low-resource setting, the novel approaches outperform existing ones on all languages by up to 11.4% accuracy. However, while accuracy in emulated low-resource scenarios is over 50% for all languages, for the truly low-resource languages Popoluca and Tepehua, our best model only obtains 37.4% and 28.4% accuracy, respectively. Thus, we conclude that canonical segmentation is still a challenging task for low-resource languages."
Targeted Finetuning for NMT with Conditional Generative-Discriminative Loss,"['Prathyusha Jwalapuram', 'Shafiq Joty', 'Youlin Shen']",,
Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network,"['Sihan Wang', 'kaijie zhou', 'Kunfeng Lai', 'Jianping Shen']",,
"Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis","['Xiaoyu Xing', 'Zhijing Jin', 'Di Jin', 'Bingning Wang', 'Qi Zhang', 'Xuanjing Huang']",http://arxiv.org/abs/2009.07964v3,"Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect's sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models' performance on ARTS by up to 32.85%. Our code and new test set are available at https://github.com/zhijing-jin/ARTS_TestSet"
TeaForN: Teacher-Forcing with N-grams,"['Sebastian Goodman', 'Nan Ding', 'Radu Soricut']",http://arxiv.org/abs/2010.03494v1,"Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps. Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model parameter updates based on N prediction steps. TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup. Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword."
TED-CDB: A Large-Scale Chinese Discourse Relation Dataset on TED Talks,"['Wanqiu Long', 'Bonnie Webber', 'Deyi Xiong']",,
Tell Me How to Ask Again: Question Data Augmentation with Controllable Rewriting in Continuous Space,"['Dayiheng Liu', 'Yeyun Gong', 'Jie Fu', 'Yu Yan', 'Jiusheng Chen', 'Jiancheng Lv', 'Nan Duan', 'Ming Zhou']",http://arxiv.org/abs/2010.01475v1,"In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks. We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples. CRQDA utilizes a Transformer autoencoder to map the original discrete question into a continuous embedding space. It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization. Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data. Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA"
TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion,"['Jiapeng Wu', 'Meng Cao', 'Jackie Chi Kit Cheung', 'William L. Hamilton']",http://arxiv.org/abs/2010.03526v1,"Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task. Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations. However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions. Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs. We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques. Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks. Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings."
Template Guided Text Generation for Task Oriented Dialogue,"['Mihir Kale', 'Abhinav Rastogi']",,
Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols,"['Prachi Jain', 'Sushant Rathi', 'Mausam -', 'Soumen Chakrabarti']",http://arxiv.org/abs/2005.05035v1,"Temporal knowledge bases associate relational (s,r,o) triples with a set of times (or a single time instant) when the relation is valid. While time-agnostic KB completion (KBC) has witnessed significant research, temporal KB completion (TKBC) is in its early days. In this paper, we consider predicting missing entities (link prediction) and missing time intervals (time prediction) as joint TKBC tasks where entities, relations, and time are all embedded in a uniform, compatible space. We present TIMEPLEX, a novel time-aware KBC method, that also automatically exploits the recurrent nature of some relations and temporal interactions between pairs of relations. TIMEPLEX achieves state-of-the-art performance on both prediction tasks. We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms. In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions."
TernaryBERT: Distillation-aware Ultra-low Bit BERT,"['Wei Zhang', 'Lu Hou', 'Yichun Yin', 'Lifeng Shang', 'Xiao Chen', 'Xin Jiang', 'Qun Liu']",http://arxiv.org/abs/2009.12812v2,"Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks.However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices. In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model. Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT. Moreover, to reduce the accuracy degradation caused by the lower capacity of low bits, we leverage the knowledge distillation technique in the training process. Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller."
TESA: A Task in Entity Semantic Aggregation for Abstractive Summarization,"['Cl√©ment Jumel', 'Annie Louis', 'Jackie Chi Kit Cheung']",,
Text Segmentation by Cross Segment Attention,"['Michal Lukasik', 'Boris Dadachev', 'Kishore Papineni', 'Goncalo Simoes']",http://arxiv.org/abs/2004.14535v1,"Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications."
Textual Data Augmentation for Efficient Active Learning on Tiny Datasets,"['Husam Quteineh', 'Spyridon Samothrakis', 'Richard Sutcliffe']",,
"The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions","['Xiang Zhou', 'Yixin Nie', 'Hao Tan', 'Mohit Bansal']",http://arxiv.org/abs/2004.13606v1,"We find that the performance of state-of-the-art models on Natural Language Inference (NLI) and Reading Comprehension (RC) analysis/stress sets can be highly unstable. This raises three questions: (1) How will the instability affect the reliability of the conclusions drawn based on these analysis sets? (2) Where does this instability come from? (3) How should we handle this instability and what are some potential solutions? For the first question, we conduct a thorough empirical study over analysis sets and find that in addition to the unstable final performance, the instability exists all along the training curve. We also observe lower-than-expected correlations between the analysis validation set and standard validation set, questioning the effectiveness of the current model-selection routine. Next, to answer the second question, we give both theoretical explanations and empirical evidence regarding the source of the instability, demonstrating that the instability mainly comes from high inter-example correlations within analysis sets. Finally, for the third question, we discuss an initial attempt to mitigate the instability and suggest guidelines for future work such as reporting the decomposed variance for more interpretable results and fair comparison across models. Our code is publicly available at: https://github. com/owenzx/InstabilityAnalysis"
The Grammar of Emergent Languages,"['Oskar van der Wal', 'Silvan de Boer', 'Elia Bruni', 'Dieuwke Hupkes']",http://arxiv.org/abs/2010.02069v1,"In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers."
The Secret is in the Spectra: Predicting Cross-lingual Task Performance with Spectral Similarity Measures,"['Haim Dubossarsky', 'Ivan Vuliƒá', 'Roi Reichart', 'Anna Korhonen']",,
The World is not Binary: Learning to Rank with Grayscale Data for Dialogue Response Selection,"['Zibo Lin', 'Deng Cai', 'Yan Wang', 'Xiaojiang Liu', 'Haitao Zheng', 'Shuming Shi']",http://arxiv.org/abs/2004.02421v3,"Response selection plays a vital role in building retrieval-based conversation systems. Despite that response selection is naturally a learning-to-rank problem, most prior works take a point-wise view and train binary classifiers for this task: each response candidate is labeled either relevant (one) or irrelevant (zero). On the one hand, this formalization can be sub-optimal due to its ignorance of the diversity of response quality. On the other hand, annotating grayscale data for learning-to-rank can be prohibitively expensive and challenging. In this work, we show that grayscale data can be automatically constructed without human effort. Our method employs off-the-shelf response retrieval models and response generation models as automatic grayscale data generators. With the constructed grayscale data, we propose multi-level ranking objectives for training, which can (1) teach a matching model to capture more fine-grained context-response relevance difference and (2) reduce the train-test discrepancy in terms of distractor strength. Our method is simple, effective, and universal. Experiments on three benchmark datasets and four state-of-the-art matching models show that the proposed approach brings significant and consistent performance improvements."
To Schedule or not to Schedule: Extracting Task Specific Temporal Entities and Associated Negation Constraints,"['Barun Patra', 'Chala Fufa', 'Pamela Bhattacharya', 'Charles Lee']",,
TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue,"['Chien-Sheng Wu', 'Steven C.H. Hoi', 'Richard Socher', 'Caiming Xiong']",http://arxiv.org/abs/2004.06871v3,"The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue."
Token-level Adaptive Training for Neural Machine Translation,"['Shuhao Gu', 'Jinchao Zhang', 'Fandong Meng', 'Yang Feng', 'Wanying Xie', 'Jie Zhou', 'Dong Yu']",,
Top-Rank-Focused Adaptive Vote Collection for the Evaluation of Domain-Specific Semantic Models,"['Pierangelo Lombardo', 'Alessio Boiardi', 'Luca Colombo', 'Angelo Schiavone', 'Nicol√≤ Tamagnone']",,
TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions,"['Qiang Ning', 'Hao Wu', 'Rujun Han', 'Nanyun Peng', 'Matt Gardner', 'Dan Roth']",http://arxiv.org/abs/2005.00242v2,"A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as ""what happened before/after [some event]?"" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance."
ToTTo: A Controlled Table-To-Text Generation Dataset,"['Ankur Parikh', 'Xuezhi Wang', 'Sebastian Gehrmann', 'Manaal Faruqui', 'Bhuwan Dhingra', 'Diyi Yang', 'Dipanjan Das']",,
Towards Debiasing NLU Models from Unknown Biases,"['Prasetya Ajie Utama', 'Nafise Sadat Moosavi', 'Iryna Gurevych']",http://arxiv.org/abs/2009.12303v2,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be \emph{known} a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness."
Towards Detecting and Exploiting Disambiguation Biases in Neural Machine Translation,"['Denis Emelin', 'Ivan Titov', 'Rico Sennrich']",,
Towards Enhancing Faithfulness for Neural Machine Translation,"['Rongxiang Weng', 'Heng Yu', 'Xiangpeng Wei', 'Weihua Luo']",,
Towards Interpretable Reasoning over Paragraph Effects in Situation,"['Mucheng Ren', 'Xiubo Geng', 'Tao QIN', 'Heyan Huang', 'Daxin Jiang']",http://arxiv.org/abs/2010.01272v1,"We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation. Existing works ignore the complicated reasoning process and solve it with a one-step ""black box"" model. Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules. In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model. Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach."
Towards Medical Machine Reading Comprehension with Structural Knowledge and Plain Text,"['Dongfang Li', 'Baotian Hu', 'Qingcai Chen', 'Weihua Peng', 'Anqi Wang']",,
Towards More Accurate Uncertainty Estimation In Text Classification,"['Jianfeng He', 'Xuchao Zhang', 'Shuo Lei', 'Zhiqian Chen', 'Fanglan Chen', 'Abdulaziz Alhamadani', 'Bei Xiao', 'ChangTien Lu']",,
Towards Persona-Based Empathetic Conversational Models,"['Peixiang Zhong', 'Chen Zhang', 'Hao Wang', 'Yong Liu', 'Chunyan Miao']",http://arxiv.org/abs/2004.12316v6,"Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains. In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy. In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations. To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding. Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations. We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset. Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding. Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations."
Training for Gibbs Sampling on Conditional Random Fields with Neural Scoring Factors,"['Sida Gao', 'Matthew R. Gormley']",,
Training Question Answering Models From Synthetic Data,"['Raul Puri', 'Ryan Spring', 'Mohammad Shoeybi', 'Mostofa Patwary', 'Bryan Catanzaro']",http://arxiv.org/abs/2002.09599v1,"Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data."
Transformer Based Multi-Source Domain Adaptation,"['Dustin Wright', 'Isabelle Augenstein']",http://arxiv.org/abs/2009.07806v1,"In practical machine learning settings, the data on which a model must make predictions often come from a different distribution than the data it was trained on. Here, we investigate the problem of unsupervised multi-source domain adaptation, where a model is trained on labelled data from multiple source domains and must make predictions on a domain for which no labelled data has been seen. Prior work with CNNs and RNNs has demonstrated the benefit of mixture of experts, where the predictions of multiple domain expert classifiers are combined; as well as domain adversarial training, to induce a domain agnostic representation space. Inspired by this, we investigate how such methods can be effectively applied to large pretrained transformer models. We find that domain adversarial training has an effect on the learned representations of these models while having little effect on their performance, suggesting that large transformer-based models are already relatively robust across domains. Additionally, we show that mixture of experts leads to significant performance improvements by comparing several variants of mixing functions, including one novel mixture based on attention. Finally, we demonstrate that the predictions of large pretrained transformer based domain experts are highly homogenous, making it challenging to learn effective functions for mixing their predictions."
Translation Artifacts in Cross-lingual Transfer Learning,"['Mikel Artetxe', 'Gorka Labaka', 'Eneko Agirre']",http://arxiv.org/abs/2004.04721v3,"Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively."
Translationese in Machine Translation Evaluation,"['Yvette Graham', 'Barry Haddow', 'Philipp Koehn']",http://arxiv.org/abs/1906.09833v1,"The term translationese has been used to describe the presence of unusual features of translated text. In this paper, we provide a detailed analysis of the adverse effects of translationese on machine translation evaluation results. Our analysis shows evidence to support differences in text originally written in a given language relative to translated text and this can potentially negatively impact the accuracy of machine translation evaluations. For this reason we recommend that reverse-created test data be omitted from future machine translation test sets. In addition, we provide a re-evaluation of a past high-profile machine translation evaluation claiming human-parity of MT, as well as analysis of the since re-evaluations of it. We find potential ways of improving the reliability of all three past evaluations. One important issue not previously considered is the statistical power of significance tests applied in past evaluations that aim to investigate human-parity of MT. Since the very aim of such evaluations is to reveal legitimate ties between human and MT systems, power analysis is of particular importance, where low power could result in claims of human parity that in fact simply correspond to Type II error. We therefore provide a detailed power analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size of translations for such studies. Subsequently, since no past evaluation that aimed to investigate claims of human parity ticks all boxes in terms of accuracy and reliability, we rerun the evaluation of the systems claiming human parity. Finally, we provide a comprehensive check-list for future machine translation evaluation."
Two are Better Than One: Joint Entity and Relation Extraction with Table-Sequence Encoders,"['Jue WANG', 'Wei Lu']",http://arxiv.org/abs/2010.03851v1,"Named entity recognition and relation extraction are two important fundamental problems. Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem. However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space. We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process. In this work, we propose the novel {\em table-sequence encoders} where two different encoders -- a table encoder and a sequence encoder are designed to help each other in the representation learning process. Our experiments confirm the advantages of having {\em two} encoders over {\em one} encoder. On several standard datasets, our model shows significant improvements over existing approaches."
Type B Reflexivization as an Unambiguous Testbed for Multilingual Multi-Task Gender Bias,"['Ana Valeria Gonz√°lez', 'Maria Barrett', 'Rasmus Hvingelby', 'Kellie Webster', 'Anders S√∏gaard']",http://arxiv.org/abs/2009.11982v2,"The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are ""hallucinatory"", e.g., disambiguating gender-ambiguous occurrences of 'doctor' as male doctors. We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of 'the doctor removed his mask' is not ambiguous between a coreferential reading and a disjoint reading. Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive. We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon. We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics."
UDapter: Language Adaptation for Truly Universal Dependency Parsing,"['Ahmet √úst√ºn', 'Arianna Bisazza', 'Gosse Bouma', 'Gertjan van Noord']",http://arxiv.org/abs/2004.14327v2,"Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, cross-language interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success."
Uncertainty-Aware Label Refinement for Sequence Labeling,"['Tao Gui', 'Jiacheng Ye', 'Qi Zhang', 'Zhengyan Li', 'Zichu Fei', 'Yeyun Gong', 'Xuanjing Huang']",,
Uncertainty-Aware Semantic Augmentation for Neural Machine Translation,"['Xiangpeng Wei', 'Heng Yu', 'Yue Hu', 'Rongxiang Weng', 'Luxi Xing', 'Weihua Luo']",,
Understanding Procedural Text using Interactive Entity Networks,"['Jizhi Tang', 'Yansong Feng', 'Dongyan Zhao']",,
Understanding the Difficulty of Training Transformers,"['Liyuan Liu', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen', 'Jiawei Han']",http://arxiv.org/abs/2004.08249v2,"Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand $\textit{what complicates Transformer training}$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin ($\textbf{Ad}$aptive $\textbf{m}$odel $\textbf{in}$itialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance. Implementations are released at: https://github.com/LiyuanLucasLiu/Transforemr-Clinic."
Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning,"['Tsvetomila Mihaylova', 'Vlad Niculae', 'Andr√© F. T. Martins']",http://arxiv.org/abs/2010.02357v1,"Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data. One challenge with end-to-end training of these models is the argmax operation, which has null gradient. In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem. We explore latent structure learning through the angle of pulling back the downstream learning objective. In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT - a variant of STE for structured models. Our perspective leads to new algorithms in the same family. We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases."
UniConv: A Unified Conversational Neural Architecture for Multi-domain Task-oriented Dialogues,"['Hung Le', 'Doyen Sahoo', 'Chenghao Liu', 'Nancy Chen', 'Steven C.H. Hoi']",http://arxiv.org/abs/2004.14307v1,"Building an end-to-end conversational agent for multi-domain task-oriented dialogue has been an open challenge for two main reasons. First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only. Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users. Unlike the existing approaches that are often designed to train each module separately, we propose ""UniConv"" -- a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously. We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines in all tasks. Our code and models will be released."
Unified Feature and Instance Based Domain adaptation for End-to-End Aspect-based Sentiment Analysis,"['Chenggong Gong', 'Jianfei Yu', 'Rui Xia']",,
UNION: An Unreferenced Metric for Evaluating Open-ended Story Generation,"['Jian Guan', 'Minlie Huang']",http://arxiv.org/abs/2009.07602v1,"Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references. To alleviate this issue, we propose UNION, a learnable unreferenced metric for evaluating open-ended story generation, which measures the quality of a generated story without any reference. Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories. We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence. Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics."
Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,"['Wenpeng Yin', 'Nazneen Fatema Rajani', 'Dragomir Radev', 'Richard Socher', 'Caiming Xiong']",http://arxiv.org/abs/2010.02584v1,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited. Code: https://github.com/salesforce/UniversalFewShotNLP"
Unsupervised Adaptation of Question Answering Systems via Generative Self-training,"['Steven Rennie', 'Etienne Marcheret', 'Neil Mallinar', 'David Nahamoo', 'Vaibhava Goel']",,
Unsupervised Commonsense Question Answering with Self-Talk,"['Vered Shwartz', 'Peter West', 'Ronan Le Bras', 'Chandra Bhagavatula', 'Yejin Choi']",http://arxiv.org/abs/2004.05483v2,"Natural language understanding involves reading between the lines with implicit background knowledge. Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge. We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks. Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as ""$\textit{what is the definition of ...}$"" to discover additional background knowledge. Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs. While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as useful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning."
Unsupervised Cross-Lingual Part-of-Speech Tagging for Truly Low-Resource Scenarios,"['Ramy Eskander', 'Smaranda Muresan', 'Michael Collins']",,
Unsupervised Discovery of Implicit Gender Bias,"['Anjalie Field', 'Yulia Tsvetkov']",http://arxiv.org/abs/2004.08361v2,"Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable. We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias. Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data. Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning. Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization. Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements."
Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning,"['Wanyun Cui', 'Guangyu Zheng', 'Wei Wang']",,
Unsupervised Parsing via Constituency Tests,"['Steven Cao', 'Nikita Kitaev', 'Dan Klein']",http://arxiv.org/abs/2010.03146v1,"We propose a method for unsupervised parsing based on the linguistic notion of a constituency test. One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical). Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions. To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score. While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model. The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previous best published result."
Unsupervised Parsing with S-DIORA: Single Tree Encoding for Deep Inside-Outside Recursive Autoencoders,"['Andrew Drozdov', 'Subendhu Rongali', 'Yi-Pei Chen', ""Tim O'Gorman"", 'Mohit Iyyer', 'Andrew McCallum']",,
Unsupervised Question Decomposition for Question Answering,"['Ethan Perez', 'Patrick Lewis', 'Wen-tau Yih', 'Kyunghyun Cho', 'Douwe Kiela']",http://arxiv.org/abs/2002.09758v3,"We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction."
Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning,"['Hanlu Wu', 'Tengfei Ma', 'Lingfei Wu', 'Tariro Manyumwa', 'Shouling Ji']",http://arxiv.org/abs/2010.01781v1,"Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task. Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary. In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning. Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT. To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss. Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries. Furthermore, we show that our method is general and transferable across datasets."
Unsupervised Stance Detection for Arguments from Consequences,"['Jonathan Kobbe', 'Ioana Hulpu»ô', 'Heiner Stuckenschmidt']",,
Variational Hierarchical Dialog Autoencoder for Dialog State Tracking Data Augmentation,"['Kang Min Yoo', 'Hanbit Lee', 'Franck Dernoncourt', 'Trung Bui', 'Walter Chang', 'Sang-goo Lee']",http://arxiv.org/abs/2001.08604v3,"Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks. In this work, we extend this approach to the task of dialog state tracking for goal-oriented dialogs. Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features. We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals. The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces. To overcome training issues that arise from training complex variational models, we propose appropriate training strategies. Experiments on various dialog datasets show that our model improves the downstream dialog trackers' robustness via generative data augmentation. We also discover additional benefits of our unified approach to modeling goal-oriented dialogs: dialog response generation and user simulation, where our model outperforms previous strong baselines."
VCDM: Leveraging Variational Bi-encoding and Deep Contextualized Word Representations for Improved Definition Modeling,"['Machel Reid', 'Edison Marrese-Taylor', 'Yutaka Matsuo']",http://arxiv.org/abs/2010.03124v1,"In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases. Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way. To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition. We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance. Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, ""Cambridge"" and the first non-English corpus ""Robert"", which we release to complement our empirical study. Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach."
VD-BERT: A Unified Vision and Dialog Transformer with BERT,"['Yue Wang', 'Shafiq Joty', 'Michael Lyu', 'Irwin King', 'Caiming Xiong', 'Steven C.H. Hoi']",http://arxiv.org/abs/2004.13278v2,"Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history. Prior work has mostly focused on various attention mechanisms to model such intricate interactions. By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks. The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture. More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training. Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard."
Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications,"['Matthew Khoury', 'Rumen Dangovski', 'Longwu Ou', 'Preslav Nakov', 'Yichen Shen', 'Li Jing']",,
Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning,"['Zhiyuan Fang', 'Tejas Gokhale', 'Pratyay Banerjee', 'Chitta Baral', 'Yezhou Yang']",http://arxiv.org/abs/2003.05162v2,"Captioning is a crucial and challenging task for video understanding. In videos that involve active agents such as humans, the agent's actions can bring about myriad changes in the scene. These changes can be observable, such as movements, manipulations, and transformations of the objects in the scene -- these are reflected in conventional video captioning. However, unlike images, actions in videos are also inherently linked to social and commonsense aspects such as intentions (why the action is taking place), attributes (such as who is doing the action, on whom, where, using what etc.) and effects (how the world changes due to the action, the effect of the action on other agents). Thus for video understanding, such as when captioning videos or when answering question about videos, one must have an understanding of these commonsense aspects. We present the first work on generating \textit{commonsense} captions directly from videos, in order to describe latent aspects such as intentions, attributes, and effects. We present a new dataset ""Video-to-Commonsense (V2C)"" that contains 9k videos of human agents performing various actions, annotated with 3 types of commonsense descriptions. Additionally we explore the use of open-ended video-based commonsense question answering (V2C-QA) as a way to enrich our captions. We finetune our commonsense generation models on the V2C-QA task where we ask questions about the latent aspects in the video. Both the generation task and the QA task can be used to enrich video captions."
Visually Grounded Compound PCFGs,"['Yanpeng Zhao', 'Ivan Titov']",http://arxiv.org/abs/2009.12404v1,"Exploiting visual groundings for language understanding has recently been drawing much attention. In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings. Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences. While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs). This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy. We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning. Additionally, this enables us to complement the image-text alignment loss with a language modeling objective. On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction. It also substantially outperforms the previous grounded model, with largest improvements on more `abstract' categories (e.g., +55.1% recall on VPs)."
Visually Grounded Continual Learning of Compositional Phrases,"['Xisen Jin', 'Junyi Du', 'Arka Sadhu', 'Ram Nevatia', 'Xiang Ren']",http://arxiv.org/abs/2005.00785v4,"Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems. To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes. In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets. VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions). To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods. Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible. We conduct further ablations and analysis to guide future work."
VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles,"['Mingzhe Li', 'Xiuying Chen', 'Shen Gao', 'Zhangming Chan', 'Dongyan Zhao', 'Rui Yan']",,
"Vokenization: Improving Language Understanding via Contextualized, Visually-Grounded Supervision","['Hao Tan', 'Mohit Bansal']",,
VolTAGE: Volatility forecasting via Text-Audio fusion with Graph convolution networks for Earnings calls,"['Ramit Sawhney', 'Piyush Khanna', 'Arshiya Aggarwal', 'Taru Jain', 'Puneet Mathur', 'Rajiv Ratn Shah']",,
Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains,"['Weijie Yu', 'Chen Xu', 'Jun Xu', 'Liang Pang', 'Xiaopeng Gao', 'Xiaozhao Wang', 'Ji-Rong Wen']",,
We Can Detect Your Bias: Predicting the Political Ideology of News Articles,"['Ramy Baly', 'Giovanni Da San Martino', 'James Glass', 'Preslav Nakov']",,
Weakly Supervised Learning of Nuanced Frames for Analyzing Polarization in News Media,"['Shamik Roy', 'Dan Goldwasser']",http://arxiv.org/abs/2009.09609v1,"In this paper we suggest a minimally-supervised approach for identifying nuanced frames in news article coverage of politically divisive topics. We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way. We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control and abortion. We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media."
Weakly Supervised Subevent Knowledge Acquisition,"['Wenlin Yao', 'Zeyu Dai', 'Maitreyi Ramaswamy', 'Bonan Min', 'Ruihong Huang']",,
Weakly-Supervised Text Classification Using Label Names Only,"['Yu Meng', 'Yunyi Zhang', 'Jiaxin Huang', 'Chenyan Xiong', 'Heng Ji', 'Chao Zhang', 'Jiawei Han']",,
What Can We Learn from Collective Human Opinions on Natural Language Inference Data?,"['Yixin Nie', 'Xiang Zhou', 'Mohit Bansal']",http://arxiv.org/abs/2010.03532v1,"Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at https://github.com/easonnie/ChaosNLI"
What do Models Learn from Question Answering Datasets?,"['Priyanka Sen', 'Amir Saffari']",http://arxiv.org/abs/2004.03490v1,"While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate what models are really learning from QA datasets by evaluating BERT-based models across five popular QA datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect information in datasets, and ability to handle variations in questions. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering."
What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding,"['Yu-An Wang', 'Yun-Nung Chen']",,
What Do You Mean by That? - A Parser-Independent Interactive Approach for Enhancing Text-to-SQL,"['Yuntao Li', 'Bei Chen', 'Qian Liu', 'Yan Gao', 'Jian-Guang LOU', 'Yan Zhang', 'Dongmei Zhang']",,
What Have We Achieved on Text Summarization?,"['Dandan Huang', 'Leyang Cui', 'Sen Yang', 'Guangsheng Bao', 'Wang Kun', 'Jun Xie', 'Yue Zhang']",,
What is More Likely to Happen Next? Video-and-Language Future Event Prediction,"['Jie Lei', 'Licheng Yu', 'Tamara Berg', 'Mohit Bansal']",,
What time is it? Temporal Analysis of Novels,"['Allen Kim', 'Charuta Pethe', 'Steve Skiena']",,
"When BERT Plays the Lottery, All Tickets Are Winning","['Sai Prasanna', 'Anna Rogers', 'Anna Rumshisky']",http://arxiv.org/abs/2005.00561v1,"Much of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the ""bad"" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the ""good"" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the ""good"" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time."
When Hearst Is not Enough: Improving Hypernymy Detection from Corpus with Distributional Models,"['Changlong Yu', 'Jialong Han', 'Peifeng Wang', 'Yangqiu Song', 'Hongming Zhang', 'Wilfred Ng', 'Shuming Shi']",,
Where are the Facts? Searching for Fact-checked Information to Alleviate the Spread of Fake News,"['Nguyen Vo', 'Kyumin Lee']",http://arxiv.org/abs/2010.03159v1,"Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media. These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation. How can we use fact-checked information to improve users' consciousness of fake news to which they are exposed? How can we stop users from spreading fake news? To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users. The search can directly warn fake news posters and online users (e.g. the posters' followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media. Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets. Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020."
Where Are You? Localization from Embodied Dialog,"['Meera Hahn', 'Jacob Krantz', 'Dhruv Batra', 'Devi Parikh', 'James Rehg', 'Stefan Lee', 'Peter Anderson']",,
Which *BERT? A Survey Organizing Contextualized Encoders,"['Patrick Xia', 'Shijie Wu', 'Benjamin Van Durme']",,
Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements,"['Yang Li', 'Gang Li', 'Luheng He', 'Jingjie Zheng', 'Hong Li', 'Zhiwei Guan']",,
Will I Sound Like Me? Improving Persona Consistency in Dialogues through Pragmatic Self-Consciousness,"['Hyunwoo Kim', 'Byeongchang Kim', 'Gunhee Kim']",http://arxiv.org/abs/2004.05816v2,"We explore the task of improving persona consistency of dialogue agents. Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency. However, such additional labels and training can be demanding. Also, we find even the best-performing persona-based agents are insensitive to contradictory words. Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener. Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction. We further extend the framework by learning the distractor selection, which has been usually done manually or randomly. Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models. Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues."
With Little Power Comes Great Responsibility,"['Dallas Card', 'Peter Henderson', 'Urvashi Khandelwal', 'Robin Jia', 'Kyle Mahowald', 'Dan Jurafsky']",,
With More Contexts Comes Better Performance: Contextualized Sense Embeddings for All-Round Word Sense Disambiguation,"['Bianca Scarlini', 'Tommaso Pasini', 'Roberto Navigli']",,
Word class flexibility: A deep contextualized approach,"['Bai Li', 'Guillaume Thomas', 'Yang Xu', 'Frank Rudzicz']",http://arxiv.org/abs/2009.09241v1,"Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories. Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties. We propose a principled methodology to explore regularity in word class flexibility. Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages. We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages. Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process. Our work highlights the utility of deep contextualized models in linguistic typology."
Word Rotator's Distance,"['Sho Yokoi', 'Ryo Takahashi', 'Reina Akama', 'Jun Suzuki', 'Kentaro Inui']",http://arxiv.org/abs/2004.15003v2,"One key principle for assessing textual similarity is measuring the degree of semantic overlap between two texts by considering the word alignment. Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors. To remedy this, we focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity. Alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance. Accordingly, we propose to decouple word vectors into their norm and direction then computing the alignment-based similarity using earth mover's distance (optimal transport cost), which we refer to as word rotator's distance. Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method. On several STS benchmarks, our simple proposed methods outperformed not only alignment-based approaches but also strong baselines."
Writing Strategies for Science Communication: Data and Computational Analysis,"['Tal August', 'Lauren Kim', 'Katharina Reinecke', 'Noah A. Smith']",,
X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models,"['Zhengbao Jiang', 'Antonios Anastasopoulos', 'Jun Araki', 'Haibo Ding', 'Graham Neubig']",,
"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers","['Jaemin Cho', 'jiasen lu', 'Dustin Schwenk', 'Hannaneh Hajishirzi', 'Aniruddha Kembhavi']",http://arxiv.org/abs/2009.11278v1,"Mirroring the success of masked language models, vision-and-language counterparts like ViLBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding. Recent work has also successfully adapted such models towards the generative task of image captioning. This begs the question: Can these models go the other way and generate images from pieces of text? Our analysis of a popular representative from this model family - LXMERT - finds that it is unable to generate rich and semantically meaningful imagery with its current training setup. We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint. X-LXMERT's image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT. Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER."
X-SRL: A Parallel Cross-Lingual Semantic Role Labeling Dataset,"['Angel Daza', 'Anette Frank']",,
XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning,"['Edoardo Maria Ponti', 'Goran Glava≈°', 'Olga Majewska', 'Qianchu Liu', 'Ivan Vuliƒá', 'Anna Korhonen']",http://arxiv.org/abs/2005.00333v1,"In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at github.com/cambridgeltl/xcopa."
"XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation","['Yaobo Liang', 'Nan Duan', 'Yeyun Gong', 'Ning Wu', 'Fenfei Guo', 'Weizhen Qi', 'Ming Gong', 'Linjun Shou', 'Daxin Jiang', 'Guihong Cao', 'Xiaodong Fan', 'Ruofei Zhang', 'Rahul Agrawal', 'Edward Cui', 'Sining Wei', 'Taroon Bharti', 'Ying Qiao', 'Jiun-Hung Chen', 'Winnie Wu', 'Shuguang Liu', 'Fan Yang', 'Daniel Campos', 'Rangan Majumder', 'Ming Zhou']",http://arxiv.org/abs/2004.01401v3,"In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE(Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder(Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison."
XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization,"['Alessandro Raganato', 'Tommaso Pasini', 'Jose Camacho-Collados', 'Mohammad Taher Pilehvar']",,
XXXXX: A Neural Framework for MT Evaluation,"['Ricardo Rei', 'Craig Stewart', 'Ana C Farinha', 'Alon Lavie']",,
Zero-Shot Cross-Lingual Transfer with Meta Learning,"['Farhad Nooralahzadeh', 'Giannis Bekoulis', 'Johannes Bjerva', 'Isabelle Augenstein']",http://arxiv.org/abs/2003.02739v4,"Learning what to share between tasks has been a topic of great importance recently, as strategic sharing of knowledge has been shown to improve downstream task performance. This is particularly important for multilingual applications, as most languages in the world are under-resourced. Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English. We show that this challenging setup can be approached using meta-learning, where, in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first. We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering). Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages. We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset). A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial."
Zero-Shot Crosslingual Sentence Simplification,"['Jonathan Mallinson', 'Rico Sennrich', 'Mirella Lapata']",,
